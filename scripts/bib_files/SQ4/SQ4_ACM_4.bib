@proceedings{10.1145/3543434,
title = {dg.o '22: Proceedings of the 23rd Annual International Conference on Digital Government Research},
year = {2022},
isbn = {9781450397490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Republic of Korea}
}

@proceedings{10.1145/3643650,
title = {SaT-CPS '24: Proceedings of the 2024 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems},
year = {2024},
isbn = {9798400705557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the fourth ACM Workshop on Secure and Trustworthy Cyber-Physical Systems (SaT-CPS 2024) held in conjunction with the 14th ACM Conference on Data and Application Security and Privacy (CODASPY 2024). ACM SaT-CPS aims to represent a forum for researchers and practitioners from industry and academia interested in various areas of CPS security. SaTCPS features novel submissions describing practical and theoretical solutions for cyber security challenges in CPS. Cyber-physical systems (CPS) entail seamless integration of computation and physical components. These systems illustrate the synergistic interactions among the cyber components, such as the computing and communication parts, and the physical devices, operating at wide varieties of spatial and temporal time scales. CPS is driving innovation and competition in a range of sectors, including agriculture, aeronautics, building design, civil infrastructure, energy, environmental quality, healthcare and personalized medicine, and transportation. These applications will empower the true vision of CPS allowing human beings to interact with the physical world and serve critical functions in our lives. CPS technologies are emerging to be the key drivers for future autonomous and smart connected worlds. With the wider adoption and popularity of the CPS applications, securing them against malicious activities is paramount. Otherwise, malfunctioning and insecure CPS devices and applications can cause enormous damage to individuals, businesses, and nations.},
location = {Porto, Portugal}
}

@article{10.5555/3322706.3362014,
author = {Durmus, Alain and Majewski, Szymon and Miasojedow, B\l{}a\.{z}ej},
title = {Analysis of langevin monte carlo via convex optimization},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {In this paper, we provide new insights on the Unadjusted Langevin Algorithm. We show that this method can be formulated as the first order optimization algorithm for an objective functional defined on the Wasserstein space of order 2. Using this interpretation and techniques borrowed from convex optimization, we give a non-asymptotic analysis of this method to sample from log-concave smooth target distribution on Rd. Based on this interpretation, we propose two new methods for sampling from a non-smooth target distribution. These new algorithms are natural extensions of the Stochastic Gradient Langevin Dynamics (SGLD) algorithm, which is a popular extension of the Unadjusted Langevin Algorithm for largescale Bayesian inference. Using the optimization perspective, we provide non-asymptotic convergence analysis for the newly proposed methods.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2666–2711},
numpages = {46},
keywords = {Bayesian inference, Wasserstein metric, convex optimization, gradient flow, unadjasted langevin algorithm}
}

@article{10.1145/3610228,
author = {Yan, Bo and Yang, Cheng and Shi, Chuan and Fang, Yong and Li, Qi and Ye, Yanfang and Du, Junping},
title = {Graph Mining for Cybersecurity: A Survey},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3610228},
doi = {10.1145/3610228},
abstract = {The explosive growth of cyber attacks today, such as malware, spam, and intrusions, has caused severe consequences on society. Securing cyberspace has become a great concern for organizations and governments. Traditional machine learning based methods are extensively used in detecting cyber threats, but they hardly model the correlations between real-world cyber entities. In recent years, with the proliferation of graph mining techniques, many researchers have investigated these techniques for capturing correlations between cyber entities and achieving high performance. It is imperative to summarize existing graph-based cybersecurity solutions to provide a guide for future studies. Therefore, as a key contribution of this work, we provide a comprehensive review of graph mining for cybersecurity, including an overview of cybersecurity tasks, the typical graph mining techniques, and the general process of applying them to cybersecurity, as well as various solutions for different cybersecurity tasks. For each task, we probe into relevant methods and highlight the graph types, graph approaches, and task levels in their modeling. Furthermore, we collect open datasets and toolkits for graph-based cybersecurity. Finally, we present an outlook on the potential directions of this field for future research.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {nov},
articleno = {47},
numpages = {52},
keywords = {Cybersecurity, cyber attack, graph mining, graph embedding, graph neural network}
}

@inproceedings{10.1145/3628797.3628945,
author = {L\^{e} Hundefinedng, Bundefinedng and L\^{e} undefinedundefinedc, Thundefinedng and undefinedo\`{a}n Minh, Trung and Trundefinedn Tuundefinedn, Dundefinedng and Phan Thundefined, Duy and Phundefinedm V\u{a}n, Hundefinedu},
title = {Contextual Language Model and Transfer Learning for Reentrancy Vulnerability Detection in Smart Contracts},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628797.3628945},
doi = {10.1145/3628797.3628945},
abstract = {The proliferation of smart contracts on blockchain technology has led to several security vulnerabilities, causing significant financial losses and instability in the contract layer. Existing machine learning-based static analysis tools have limited detection accuracy, even for known vulnerabilities. In this study, we propose a novel deep learning-based model combined with attention mechanisms for identifying security vulnerabilities in smart contracts. Our experiments on two large datasets (SmartBugs Wild and Slither Audited Smart Contracts) demonstrate that our approach successfully achieves a 90\% detection accuracy in identifying smart contract reentrancy attacks (e.g. performing better than other existing state-of-the-art deep learning-based approaches). In addition, this work also establishes the practical application of deep learning-based technology in smart contract reentrancy vulnerability detection, which can promote future research in this domain.},
booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology},
pages = {739–745},
numpages = {7},
keywords = {Blockchain, Deep learning, Language model, Reentrancy vulnerability, Smart contract},
location = {Ho Chi Minh, Vietnam},
series = {SOICT '23}
}

@article{10.1145/3657284,
author = {Shen, Meng and Tan, Zhehui and Niyato, Dusit and Liu, Yuzhi and Kang, Jiawen and Xiong, Zehui and Zhu, Liehuang and Wang, Wei and Shen, Xuemin (Sherman)},
title = {Artificial Intelligence for Web 3.0: A Comprehensive Survey},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3657284},
doi = {10.1145/3657284},
abstract = {Web 3.0 is the next generation of the Internet built on decentralized technologies such as blockchain and cryptography. It is born to solve the problems faced by the previous generation of the Internet such as imbalanced distribution of interests, monopoly of platform resources, and leakage of personal privacy. In this survey, we discuss the latest development status of Web 3.0 and the application of emerging AI technologies in it. First, we investigate the current successful practices of Web 3.0 and various components in the current Web 3.0 ecosystem and thus propose the hierarchical architecture of the Web 3.0 ecosystem from the perspective of application scenarios. The architecture we proposed contains four layers: data management, value circulation, ecological governance, and application scenarios. We dive into the current state of development and the main challenges and issues present in each layer. In this context, we find that AI technology will have great potential. We first briefly introduce the role that artificial intelligence technology may play in the development of Web 3.0. Then, we conduct an in-depth analysis of the current application status of artificial intelligence technology in the four layers of Web 3.0 and provide some insights into its potential future development directions.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {247},
numpages = {39},
keywords = {Web 3.0, artificial intelligence, blockchain, computing network}
}

@article{10.1145/3488245,
author = {Chen, Jiachi and Xia, Xin and Lo, David and Grundy, John},
title = {Why Do Smart Contracts Self-Destruct? Investigating the Selfdestruct Function on Ethereum},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3488245},
doi = {10.1145/3488245},
abstract = {The selfdestruct function is provided by Ethereum smart contracts to destroy a contract on the blockchain system. However, it is a double-edged sword for developers. On the one hand, using the selfdestruct function enables developers to remove smart contracts (SCs) from Ethereum and transfers Ethers when emergency situations happen, e.g., being attacked. On the other hand, this function can increase the complexity for the development and open an attack vector for attackers. To better understand the reasons why SC developers include or exclude the selfdestruct function in their contracts, we conducted an online survey to collect feedback from them and summarize the key reasons. Their feedback shows that 66.67\% of the developers will deploy an updated contract to the Ethereum after destructing the old contract. According to this information, we propose a method to find the self-destructed contracts (also called predecessor contracts) and their updated version (successor contracts) by computing the code similarity. By analyzing the difference between the predecessor contracts and their successor contracts, we found five reasons that led to the death of the contracts; two of them (i.e., Unmatched ERC20 Token and Limits of Permission) might affect the life span of contracts. We developed a tool named LifeScope to detect these problems. LifeScope reports 0 false positives or negatives in detecting Unmatched ERC20 Token. In terms of Limits of Permission, LifeScope achieves 77.89\% of F-measure and 0.8673 of AUC in average. According to the feedback of developers who exclude selfdestruct functions, we propose suggestions to help developers use selfdestruct functions in Ethereum smart contracts better.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {30},
numpages = {37},
keywords = {Smart contract, ethereum, selfdestruct function, empirical study}
}

@inproceedings{10.1145/3597503.3623340,
author = {Jamieson, Jack and Yamashita, Naomi and Foong, Eureka},
title = {Predicting open source contributor turnover from value-related discussions: An analysis of GitHub issues},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623340},
doi = {10.1145/3597503.3623340},
abstract = {Discussions about project values are important for engineering software that meets diverse human needs and positively impacts society. Because value-related discussions involve deeply held beliefs, they can lead to conflicts or other outcomes that may affect motivations to continue contributing to open source projects. However, it is unclear what kind of value-related discussions are associated with significant changes in turnover. We address this gap by identifying discussions related to important project values and investigating the extent to which those discussions predict project turnover in the following months. We collected logs of GitHub issues and commits from 52 projects that share similar ethical commitments and were identified as part of the DWeb (Decentralized Web) community. We identify issues related to DWeb's core values of respectfulness, freedom, broadmindedness, opposing centralized social power, equity \&amp; equality, and protecting the environment. We then use Granger causality analysis to examine how changes in the proportion of discussions related to those values might predict changes in incoming and outgoing turnover. We found multiple significant relationships between value-related discussions and turnover, including that discussions about respectfulness predict an increase in contributors leaving and a decrease in new contributors, while discussions about social power predicted better contributor retention. Understanding these antecedents of contributor turnover is important for managing open source projects that incorporate human-centric issues. Based on the results, we discuss implications for open source maintainers and for future research.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {57},
numpages = {13},
keywords = {human values, turnover, open source, GitHub},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/2542050.2542082,
author = {Nga, Nguyen Thi Thanh and Ngo, Son Hong and Thu, Ngo Quynh},
title = {Correlation-based clustering in wireless sensor network for energy saving protocol},
year = {2013},
isbn = {9781450324540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2542050.2542082},
doi = {10.1145/2542050.2542082},
abstract = {Our research concentrates on the energy efficiency in Wireless Sensor Network. One approach is based on the characteristics of environment -- the correlation among sensed data of nodes in a region. The sensor nodes are clustered into highly correlated regions (HCRs) to take advantage of correlation between sensor nodes in order to save energy. However, the determination of HCR is very complex in calculation, thus causes difficulty in implementation. This paper proposes a correlation-based approach that evaluates the correlation between two data sets using a simple calculation and that guarantees the accuracy in correlated evaluation between data. This correlation-based method is proposed to cluster sensor nodes into HCRs. Because of highly correlated characteristics among sensed data of nodes in the same HCRs, some high correlated-nodes would be inactive for energy saving. Simulation results show that the network lifetime of proposed system is 1.75 times longer than that of the conventional protocol.},
booktitle = {Proceedings of the 4th Symposium on Information and Communication Technology},
pages = {242–250},
numpages = {9},
keywords = {correlation ratio, energy efficiency, entropy, highly correlated region, joint entropy},
location = {Danang, Vietnam},
series = {SoICT '13}
}

@proceedings{10.1145/3658852,
title = {MOCO '24: Proceedings of the 9th International Conference on Movement and Computing},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Utrecht, Netherlands}
}

@article{10.1145/3357797,
author = {Si, Wen and Liu, Cong and Bi, Zhongqin and Shan, Meijing},
title = {Modeling Long-Term Dependencies from Videos Using Deep Multiplicative Neural Networks},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3357797},
doi = {10.1145/3357797},
abstract = {Understanding temporal dependencies of videos is fundamental for vision problems, but deep learning–based models are still insufficient in this field. In this article, we propose a novel deep multiplicative neural network (DMNN) for learning hierarchical long-term representations from video. The DMNN is built upon the multiplicative block that remembers the pairwise transformations between consecutive frames using multiplicative interactions rather than the regular weighted-sum ones. The block is slided over the timesteps to update the memory of the networks on the frame pairs. Deep architecture can be implemented by stacking multiple layers of the sliding blocks. The multiplicative interactions lead to exact, rather than approximate, modeling of temporal dependencies. The memory mechanism can remember the temporal dependencies for an arbitrary length of time. The multiple layers output multiple-level representations that reflect the multi-timescale structure of video. Moreover, to address the difficulty of training DMNNs, we derive a theoretically sound convergent method, which leads to a fast and stable convergence. We demonstrate a new state-of-the-art classification performance with proposed networks on the UCF101 dataset and the effectiveness of capturing complicate temporal dependencies on a variety of synthetic datasets.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jul},
articleno = {63},
numpages = {19},
keywords = {Deep learning, temporal dependencies, video recognition}
}

@inproceedings{10.1145/2485922.2485949,
author = {Cook, Henry and Moreto, Miquel and Bird, Sarah and Dao, Khanh and Patterson, David A. and Asanovic, Krste},
title = {A hardware evaluation of cache partitioning to improve utilization and energy-efficiency while preserving responsiveness},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485949},
doi = {10.1145/2485922.2485949},
abstract = {Computing workloads often contain a mix of interactive, latency-sensitive foreground applications and recurring background computations. To guarantee responsiveness, interactive and batch applications are often run on disjoint sets of resources, but this incurs additional energy, power, and capital costs. In this paper, we evaluate the potential of hardware cache partitioning mechanisms and policies to improve efficiency by allowing background applications to run simultaneously with interactive foreground applications, while avoiding degradation in interactive responsiveness. We evaluate these tradeoffs using commercial x86 multicore hardware that supports cache partitioning, and find that real hardware measurements with full applications provide different observations than past simulation-based evaluations. Co-scheduling applications without LLC partitioning leads to a 10\% energy improvement and average throughput improvement of 54\% compared to running tasks separately, but can result in foreground performance degradation of up to 34\% with an average of 6\%. With optimal static LLC partitioning, the average energy improvement increases to 12\% and the average throughput improvement to 60\%, while the worst case slowdown is reduced noticeably to 7\% with an average slowdown of only 2\%. We also evaluate a practical low-overhead dynamic algorithm to control partition sizes, and are able to realize the potential performance guarantees of the optimal static approach, while increasing background throughput by an additional 19\%.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {308–319},
numpages = {12},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@article{10.1145/2508148.2485949,
author = {Cook, Henry and Moreto, Miquel and Bird, Sarah and Dao, Khanh and Patterson, David A. and Asanovic, Krste},
title = {A hardware evaluation of cache partitioning to improve utilization and energy-efficiency while preserving responsiveness},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2508148.2485949},
doi = {10.1145/2508148.2485949},
abstract = {Computing workloads often contain a mix of interactive, latency-sensitive foreground applications and recurring background computations. To guarantee responsiveness, interactive and batch applications are often run on disjoint sets of resources, but this incurs additional energy, power, and capital costs. In this paper, we evaluate the potential of hardware cache partitioning mechanisms and policies to improve efficiency by allowing background applications to run simultaneously with interactive foreground applications, while avoiding degradation in interactive responsiveness. We evaluate these tradeoffs using commercial x86 multicore hardware that supports cache partitioning, and find that real hardware measurements with full applications provide different observations than past simulation-based evaluations. Co-scheduling applications without LLC partitioning leads to a 10\% energy improvement and average throughput improvement of 54\% compared to running tasks separately, but can result in foreground performance degradation of up to 34\% with an average of 6\%. With optimal static LLC partitioning, the average energy improvement increases to 12\% and the average throughput improvement to 60\%, while the worst case slowdown is reduced noticeably to 7\% with an average slowdown of only 2\%. We also evaluate a practical low-overhead dynamic algorithm to control partition sizes, and are able to realize the potential performance guarantees of the optimal static approach, while increasing background throughput by an additional 19\%.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {308–319},
numpages = {12}
}

@inproceedings{10.1145/3661638.3661685,
author = {Tang, Xin},
title = {Research on the Design of Financial Management System Based on Cloud Platform},
year = {2024},
isbn = {9798400716966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661638.3661685},
doi = {10.1145/3661638.3661685},
abstract = {With the maturity and popularization of cloud computing technology, the informationization of enterprise financial management system becomes the development trend. In this context, this article researches and designs enterprise financial management system based on cloud platform. The overall model adopts MVC pattern to develop and use Struts + Spring + lbatis and build it under the B/S framework of J2EE. The system safety management is realized by multi-functional module division. With the help of this system, it can effectively promote the smooth flow of enterprise financial information and realize fine management, which has theoretical and practical significance for the transformation of enterprise financial management to informationization, digitalization and intelligentization.},
booktitle = {Proceedings of the 2023 International Conference on Artificial Intelligence, Systems and Network Security},
pages = {233–240},
numpages = {8},
location = {Mianyang, China},
series = {AISNS '23}
}

@proceedings{10.1145/3573428,
title = {EITCE '22: Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
year = {2022},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@inproceedings{10.1145/2072298.2072349,
author = {Quynh, Dao T.P. and He, Ying and Chen, Xiaoming and Xia, Jiazhi and Sun, Qian and Hoi, Steven C.H.},
title = {Modeling 3D articulated motions with conformal geometry videos (CGVs)},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072349},
doi = {10.1145/2072298.2072349},
abstract = {3D articulated motions are widely used in entertainment, sports, military, and medical applications. Among various techniques for modeling 3D motions, geometry videos (GVs) are a compact representation in that each frame is parameterized to a 2D domain, which captures the 3D geometry (x, y, z) to a pixel (r, g, b) in the image domain. As a result, the widely studied image/video processing techniques can be directly borrowed for 3D motion. This paper presents conformal geometry videos (CGVs), a novel extension of the traditional geometry videos by taking into the consideration of the isometric nature of 3D articulated motions. We prove that the 3D articulated motion can be uniquely (up to rigid motion) represented by (»,H), where » is the conformal factor characterizing the intrinsic property of the 3D motion, and H the mean curvature characterizing the extrinsic feature (i.e., embedding or appearance). Furthermore, the conformal factor » is pose-invariant. Thus, in sharp contrast to the GVs which capture 3D motion by three channels, CGVs take only one channel of mean curvature H and the first frame of the conformal factor », i.e., approximately 1/3 the storage of the GVs. In addition, CGVs have strong spatial and temporal coherence, which favors various well studied video compression techniques. Thus, CGVs can be highly compressed by using the state-of the-art video compression techniques, such as H.264/AVC. Our experimental results on real-world 3D motions show that CGVs are a highly compact representation for 3D articulated motions, i.e., given CGVs and GVs of the same file size, CGVs show much better visual quality than GVs.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {383–392},
numpages = {10},
keywords = {3d articulated motion, conformal geometry videos, conformal parameterization, deformable objects, geometry videos, h.264/avc, isometric transformation, video compression},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@article{10.1145/3563044,
author = {Alghazwi, Mohammed and Turkmen, Fatih and Van Der Velde, Joeri and Karastoyanova, Dimka},
title = {Blockchain for Genomics: A Systematic Literature Review},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3563044},
doi = {10.1145/3563044},
abstract = {Human genomic data carry unique information about an individual and offer unprecedented opportunities for healthcare. The clinical interpretations derived from large genomic datasets can greatly improve healthcare and pave the way for personalized medicine. Sharing genomic datasets, however, poses major challenges, as genomic data is different from traditional medical data, indirectly revealing information about descendants and relatives of the data owner and carrying valid information even after the owner passes away. Therefore, stringent data ownership and control measures are required when dealing with genomic data. In order to provide a secure and accountable infrastructure, blockchain technologies offer a promising alternative to traditional distributed systems. Indeed, the research on blockchain-based infrastructures tailored to genomics is on the rise. However, there is a lack of a comprehensive literature review that summarizes the current state-of-the-art methods in the applications of blockchain in genomics. In this article, we systematically look at the existing work both commercial and academic, and discuss the major opportunities and challenges. Our study is driven by five research questions that we aim to answer in our review. We also present our projections of future research directions which we hope the researchers interested in the area can benefit from.},
journal = {Distrib. Ledger Technol.},
month = {dec},
articleno = {11},
numpages = {28},
keywords = {Blockchain, distributed ledger technology, privacy, genomics, SLR}
}

@proceedings{10.1145/3582515,
title = {GoodIT '23: Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3620666,
title = {ASPLOS '24: Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
abstract = {Welcome to the third volume of ASPLOS'24: the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. This document is mostly dedicated to the 2024 fall cycle but also provides some statistics summarizing all three cycles.We introduced several notable changes to ASPLOS this year, most of which were discussed in our previous messages from program chairs in Volume 1 and 2, including: (1) significantly increasing the program committee size to over 220 members (more than twice the size of last year); (2) foregoing synchronous program committee (PC) meetings and instead making all decisions online; (3) overhauling the review assignment process; (4) developing an automated submission format violation identifier script that uncovers, e.g., disallowed vertical space manipulations that "squeeze" space; (5) introducing the new ASPLOS role of Program Vice Chairs to cope with the increased number of submissions and the added load caused by foregoing synchronous program committee; and (6) characterizing a systematic problem that ASPLOS is facing in reviewing quantum computing submissions, describing how we addressed it and highlighting how we believe that it should be handled in the future.Assuming readers have read our previous messages, here, we will only describe differences between the current cycle and the previous ones. These include: (1) Finally unifying submission and acceptance paper formatting instructions (forgoing the `jpaper' class) to rid authors of accepted papers from the need to reformat; (2) Describing the methodology we employed to select best papers, which we believe ensures quality and hope will persist; and (3) Reporting the ethical incidents we encountered and how we handled them. In the final, fourth volume, when the outcome of the ASPLOS'24 fall major revisions will become known, we plan to conduct a broader analysis of all the data we have gathered throughout the year.Following are some key statistics of the fall cycle: 340 submissions were finalized (43\% more than last year's fall count and 17\% less than our summer cycle) of which 111 are related to accelerators/FPGAs/GPUs, 105 to machine learning, 54 to security, 50 to datacenter/cloud and 50 to storage/memory; 183 (54\%) submissions were promoted to the second review round; 39 (11.5\%) papers were accepted (of which 19 were awarded artifact evaluation badges); 33 (9.7\%) submissions were allowed to submit major revisions and are currently under review (these will be addressed in the fourth volume of ASPLOS'24 and will be presented in ASPLOS'25 if accepted); 1,368 reviews were uploaded; and 4,949 comments were generated during online discussions, of which 4,070 were dedicated to the submissions that made it to the second review round.This year, in the submission form, we asked authors to specify which of the three ASPLOS research areas are related to their submitted work. Analyzing this data revealed that 80\%, 39\%, and 29\% of the submissions are categorized by their authors as related to architecture, operating systems, and programming languages, respectively, generating the highest difference we have observed across the cycles between architecture and the other two. About 46\% of the fall submissions are "interdisciplinary," namely, were associated with two or more of the three areas.Overall, throughout all the ASPLOS'24 cycles, we received 922 submissions, constituting a 1.54x increase compared to last year. Our reviewers submitted a total of 3,634 reviews containing more than 2.6 million words, and we also generated 12,655 online comments consisting of nearly 1.2 million words. As planned, PC members submitted an average of 15.7 reviews and a median of 15, and external review committee (ERC) members submitted an average of 4.7 and a median of 5.We accepted 170 papers thus far, written by 1100 authors, leading to an 18.4\% acceptance rate, with the aforementioned 33 major revisions still under review. Assuming that the revision acceptance rate will be similar to that of previous cycles, we estimate that ASPLOS'24 will accept nearly 200 (!) papers, namely, 21\%–22\% of the submissions.The ASPLOS'24 program consists of 193 papers: the 170 papers we accepted thus far and, in addition, 23 major revisions from the fall cycle of ASPLOS'23, which were re-reviewed and accepted. The full details are available in the PDF of the front matter.},
location = {La Jolla, CA, USA}
}

@inproceedings{10.1145/3447548.3467380,
author = {Liao, Xuejun and Koch, Patrick and Huang, Shunping and Xu, Yan},
title = {Efficient Collaborative Filtering via Data Augmentation and Step-size Optimization},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467380},
doi = {10.1145/3447548.3467380},
abstract = {As a popular approach to collaborative filtering, matrix factorization (MF) models the underlying rating matrix as a product of two factor matrices, one for users and one for items. The MF model can be learned by Alternating Least Squares (ALS), which updates the two factor matrices alternately, keeping one fixed while updating the other. Although ALS improves the learning objective aggressively in each iteration, it suffers from high computational cost due to the necessity of inverting a separate matrix for every user and item. The softImpute-ALS reduces the per-iteration computation significantly using a strategy that requires only two matrix inversions; however, the computation saving leads to shrinkage of objective improvement. In this paper, we introduce a new algorithm, termed Data Augmentation with Optimal Step-size (DAOS), which alleviates the drawback of softImpute-ALS while still maintaining its low cost of computation per iteration. The DAOS is presented in the context that factor matrices may include fixed columns or rows, with this allowing bias terms and/or linear models to be incorporated into the ML model. Experimental results on synthetic and MovieLens 1M Dataset demonstrate the benefits of DAOS over ALS and softImpute-ALS in terms of generalization performance and computational time.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {1006–1016},
numpages = {11},
keywords = {alternating least squares (als), collaborative filtering, data augmentation (da), matrix factorization, optimal step-size (os), pre-defined factors, softimpute-als},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.5555/3382225.3382291,
author = {Yilmaz, Cennet Merve and Durahim, Ahmet Onur},
title = {SPR2EP: a semi-supervised spam review detection framework},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {Authenticity and reliability of the information spread over the cyberspace is becoming increasingly important. This is especially important in e-commerce since potential customers check reviews and customer feedbacks online before making a purchasing decision. Although this information is easily accessible through related websites, lack of verification of the authenticity of these reviews raises concerns about their reliability. Besides, fraudulent users disseminate misinformation to deceive people into acting against their interest. So, detection of fake and unreliable reviews is a crucial problem that must be addressed by the security researchers.Here we propose a spam review detection framework that incorporates knowledge extracted from the textual content of the reviews with information obtained by exploiting the underlying reviewer-product network structure. In the proposed framework, first, feature vectors are learned for each review, reviewer and product by utilizing state-of-the-art algorithms developed for learning document and node embeddings, and then these are fed into a classifier to identify opinion spam.The effectiveness of our framework over existing techniques on detecting spam reviews is demonstrated in three different data sets containing online reviews. The experimental results obtained confirm that combining representations learned from reviewer-product network and textual review data significantly improves the detection of spam reviews.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {306–313},
numpages = {8},
keywords = {document and node embeddings, feature learning, review spam detection, web mining},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@proceedings{10.1145/3605423,
title = {ICCTA '23: Proceedings of the 2023 9th International Conference on Computer Technology Applications},
year = {2023},
isbn = {9781450399579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@article{10.1613/jair.1.13083,
author = {Razumovskaia, Evgeniia and Glavas, Goran and Majewska, Olga and Ponti, Edoardo M. and Korhonen, Anna and Vulic, Ivan},
title = {Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems},
year = {2022},
issue_date = {Sep 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {74},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13083},
doi = {10.1613/jair.1.13083},
abstract = {In task-oriented dialogue (ToD), a user holds a conversation with an artificial agent&nbsp; with the aim of completing a concrete task. Although this technology represents one of&nbsp; the central objectives of AI and has been the focus of ever more intense research and&nbsp; development efforts, it is currently limited to a few narrow domains (e.g., food ordering,&nbsp; ticket booking) and a handful of languages (e.g., English, Chinese). This work provides an&nbsp; extensive overview of existing methods and resources in multilingual ToD as an entry point&nbsp; to this exciting and emerging field. We find that the most critical factor preventing the&nbsp; creation of truly multilingual ToD systems is the lack of datasets in most languages for&nbsp; both training and evaluation. In fact, acquiring annotations or human feedback for each&nbsp; component of modular systems or for data-hungry end-to-end systems is expensive and&nbsp; tedious. Hence, state-of-the-art approaches to multilingual ToD mostly rely on (zero- or&nbsp; few-shot) cross-lingual transfer from resource-rich languages (almost exclusively English),&nbsp; either by means of (i) machine translation or (ii) multilingual representations. These&nbsp; approaches are currently viable only for typologically similar languages and languages with&nbsp; parallel / monolingual corpora available. On the other hand, their effectiveness beyond these&nbsp; boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks&nbsp; (especially for natural language generation and end-to-end evaluation). To overcome this&nbsp; limitation, we draw parallels between components of the ToD pipeline and other NLP tasks,&nbsp; which can inspire solutions for learning in low-resource scenarios. Finally, we list additional&nbsp; challenges that multilinguality poses for related areas (such as speech, fluency in generated&nbsp; text, and human-centred evaluation), and indicate future directions that hold promise to&nbsp; further expand language coverage and dialogue capabilities of current ToD systems.&nbsp;},
journal = {J. Artif. Int. Res.},
month = {sep},
numpages = {52}
}

@article{10.1145/3546938,
author = {Gunawardena, Nishan and Ginige, Jeewani Anupama and Javadi, Bahman},
title = {Eye-tracking Technologies in Mobile Devices Using Edge Computing: A Systematic Review},
year = {2022},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3546938},
doi = {10.1145/3546938},
abstract = {Eye-tracking provides invaluable insight into the cognitive activities underlying a wide range of human behaviours. Identifying cognitive activities provides valuable perceptions of human learning patterns and signs of cognitive diseases like Alzheimer’s, Parkinson’s, and autism. Also, mobile devices have changed the way that we experience daily life and become a pervasive part. This systematic review provides a detailed analysis of mobile device eye-tracking technology reported in 36 studies published in high-ranked scientific journals from 2010 to 2020 (September), along with several reports from grey literature. The review provides in-depth analysis on algorithms, additional apparatus, calibration methods, computational systems, and metrics applied to measure the performance of the proposed solutions. Also, the review presents a comprehensive classification of mobile device eye-tracking applications used across various domains such as healthcare, education, road safety, news, and human authentication. We have outlined the shortcomings identified in the literature and the limitations of the current mobile device eye-tracking technologies, such as using the front-facing mobile camera. Further, we have proposed an edge computing driven eye-tracking solution to achieve the real-time eye-tracking experience. Based on the findings, the article outlines various research gaps and future opportunities that are expected to be of significant value for improving the work in the eye-tracking domain.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {158},
numpages = {33},
keywords = {Eye-tracking, edge computing, systematic review, mobile devices, mobile human computer interaction}
}

@proceedings{10.1145/3641142,
title = {ACSW '24: Proceedings of the 2024 Australasian Computer Science Week},
year = {2024},
isbn = {9798400717307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3492805.3492807,
author = {Tatebe, Osamu and Obata, Kazuki and Hiraga, Kohei and Ohtsuji, Hiroki},
title = {CHFS: Parallel Consistent Hashing File System for Node-local Persistent Memory},
year = {2022},
isbn = {9781450384988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492805.3492807},
doi = {10.1145/3492805.3492807},
abstract = {This paper proposes a design for CHFS, an ad hoc parallel file system that utilizes the persistent memory of compute nodes. The design is based entirely on a highly scalable distributed key-value store with consistent hashing. CHFS improves the scalability of parallel data-access performance and metadata performance in terms of the number of compute nodes by eliminating dedicated metadata servers, sequential execution, and centralized data management. The implementation efficiently utilizes multicore and manycore CPUs, high-performance networks, and remote direct memory access by the Mochi-Margo library. With a 4-node persistent memory cluster, CHFS performs 9.9 times better than the state-of-the-art DAOS distributed object storage and 6.0 times better than GekkoFS on the IOR hard write benchmark. Regarding scalability, CHFS displays better scalability and performance for both bandwidth and metadata compared with BeeOND and GekkoFS. CHFS is a promising building block for HPC storage layers.},
booktitle = {International Conference on High Performance Computing in Asia-Pacific Region},
pages = {115–124},
numpages = {10},
keywords = {Distributed key-value store, Parallel file system, Persistent memory},
location = {Virtual Event, Japan},
series = {HPCAsia '22}
}

@proceedings{10.1145/3607947,
title = {IC3-2023: Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing},
year = {2023},
isbn = {9798400700224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Noida, India}
}

@proceedings{10.1145/3588155,
title = {APIT '23: Proceedings of the 2023 5th Asia Pacific Information Technology Conference},
year = {2023},
isbn = {9781450399500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ho Chi Minh City, Vietnam}
}

@proceedings{10.1145/3657054,
title = {dg.o '24: Proceedings of the 25th Annual International Conference on Digital Government Research},
year = {2024},
isbn = {9798400709883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Taipei, Taiwan}
}

@proceedings{10.1145/3487553,
title = {WWW '22: Companion Proceedings of the Web Conference 2022},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Lyon, France}
}

@article{10.1145/3567582,
author = {Pasdar, Amirmohammad and Lee, Young Choon and Dong, Zhongli},
title = {Connect API with Blockchain: A Survey on Blockchain Oracle Implementation},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3567582},
doi = {10.1145/3567582},
abstract = {A blockchain is a form of distributed ledger technology where transactions as data state changes are permanently recorded securely and transparently without the need for third parties. Besides, introducing smart contracts to the blockchain has added programmability, revolutionizing the software ecosystem toward decentralized applications. Although promising, the usability of smart contracts is primarily limited to on-chain data without access to the external systems (i.e., off-chain) where real-world data and events reside. This connectability to off-chain data for smart contracts and blockchain is an open practical problem referred to as the “oracle problem” and is defined as how real-world data can be transferred into/from the blockchain. Hence, Blockchain oracles are introduced and implemented in the form of application programming interfaces connecting the real world to the blockchain for mitigating such a limitation. This article studies and analyzes how blockchain oracles provide final feedback (i.e., outcome) to smart contracts and survey blockchain oracle technologies and mechanisms regarding data integrity and correctness. Since the existing solutions are extensive in terms of characteristics and usage, we investigate their structure and principles by classifying the blockchain oracle implementation techniques into two major groups voting-based strategies and reputation-based ones. The former mainly relies on participants’ stakes for outcome finalization, while the latter considers reputation and performance metrics in conjunction with authenticity-proof mechanisms for data correctness and integrity. We present the result of this classification with a thorough discussion of the state of the art and provide the remaining challenges and future research directions in the end.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {208},
numpages = {39},
keywords = {Blockchain, decentralized oracle, blockchain oracle, smart contracts}
}

@article{10.1145/3570639,
author = {Xu, Jiahua and Paruch, Krzysztof and Cousaert, Simon and Feng, Yebo},
title = {SoK: Decentralized Exchanges (DEX) with Automated Market Maker (AMM) Protocols},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3570639},
doi = {10.1145/3570639},
abstract = {As an integral part of the decentralized finance (DeFi) ecosystem, decentralized exchanges (DEXs) with automated market maker (AMM) protocols have gained massive traction with the recently revived interest in blockchain and distributed ledger technology (DLT) in general. Instead of matching the buy and sell sides, automated market makers (AMMs) employ a peer-to-pool method and determine asset price algorithmically through a so-called conservation function. To facilitate the improvement and development of AMM-based decentralized exchanges (DEXs), we create the first systematization of knowledge in this area. We first establish a general AMM framework describing the economics and formalizing the system’s state-space representation. We then employ our framework to systematically compare the top AMM protocols’ mechanics, illustrating their conservation functions, as well as slippage and divergence loss functions. We further discuss security and privacy concerns, how they are enabled by AMM-based DEXs’ inherent properties, and explore mitigating solutions. Finally, we conduct a comprehensive literature review on related work covering both DeFi and conventional market microstructure.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {238},
numpages = {50},
keywords = {Decentralized finance, decentralized exchange, automated market maker, blockchain, Ethereum}
}

@proceedings{10.1145/3584748,
title = {EBIMCS '22: Proceedings of the 2022 5th International Conference on E-Business, Information Management and Computer Science},
year = {2022},
isbn = {9781450397827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hong Kong, Hong Kong}
}

@inproceedings{10.1145/3650215.3650330,
author = {Jiang, Qingbo and Huang, Yong},
title = {Neural Machine Translation Research Progress and Its Implications for Education Technology: A Bibliometric Analysis},
year = {2024},
isbn = {9798400709449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650215.3650330},
doi = {10.1145/3650215.3650330},
abstract = {Neural machine translation (NMT) stands as one of the most prominent domains in contemporary natural language processing research. In this study, employing VOSviewer software, we conducted a bibliometric analysis based on 1386 NMT-related publications retrieved from the Web of Science database as of September 2023. Co-citation analysis revealed that influential sources in the NMT field were primarily concentrated in top-tier conferences and journals in areas such as artificial intelligence, neural computing, and computer vision. Notably influential authors focused on machine translation techniques, deep learning, and natural language processing technologies. Co-occurrence analysis delineated two prominent research directions within the NMT domain: machine translation and deep learning. Based on the outcomes of bibliometric analysis, we provide insights into the current state, challenges, and prospects of NMT research and offer an understanding of NMT applications in the realm of educational technology.},
booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application},
pages = {652–657},
numpages = {6},
location = {Hangzhou, China},
series = {ICMLCA '23}
}

@proceedings{10.1145/3543712,
title = {ICCTA '22: Proceedings of the 2022 8th International Conference on Computer Technology Applications},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@article{10.1145/2049656.2049658,
author = {Albrecht, Jeannie and Tuttle, Christopher and Braud, Ryan and Dao, Darren and Topilski, Nikolay and Snoeren, Alex C. and Vahdat, Amin},
title = {Distributed application configuration, management, and visualization with plush},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/2049656.2049658},
doi = {10.1145/2049656.2049658},
abstract = {Support for distributed application management in large-scale networked environments remains in its early stages. Although a number of solutions exist for subtasks of application deployment, monitoring, and maintenance in distributed environments, few tools provide a unified framework for application management. Many of the existing tools address the management needs of a single type of application or service that runs in a specific environment, and these tools are not adaptable enough to be used for other applications or platforms. To this end, we present the design and implementation of Plush, a fully configurable application management infrastructure designed to meet the general requirements of several different classes of distributed applications. Plush allows developers to specifically define the flow of control needed by their computations using application building blocks. Through an extensible resource management interface, Plush supports execution in a variety of environments, including both live deployment platforms and emulated clusters. Plush also uses relaxed synchronization primitives for improving fault tolerance and liveness in failure-prone environments. To gain an understanding of how Plush manages different classes of distributed applications, we take a closer look at specific applications and evaluate how Plush provides support for each.},
journal = {ACM Trans. Internet Technol.},
month = {dec},
articleno = {6},
numpages = {41},
keywords = {Application management, PlanetLab}
}

@inproceedings{10.1145/2669592.2669695,
author = {Lachter, Joel and Battiste, Vernol and Matessa, Michael and Dao, Quang V. and Koteskey, Robert and Johnson, Walter W.},
title = {Toward single pilot operations: the impact of the loss of non-verbal communication on the flight deck},
year = {2014},
isbn = {9781450325608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2669592.2669695},
doi = {10.1145/2669592.2669695},
abstract = {Since the 1950s, the crew required to fly transport category aircraft has been reduced from five to two. NASA is currently exploring the feasibility of a further reduction to one pilot. In this study we examine the effects of separating the pilots on crew interaction. The results are consistent with earlier research on decision-making between remote groups. Pilots strongly prefer face-to-face interactions; however, we could find no impact of separation on their ultimate decisions. There were a number of areas in which separation negatively affected communications. We discuss possible mitigations for these areas.},
booktitle = {Proceedings of the International Conference on Human-Computer Interaction in Aerospace},
articleno = {29},
numpages = {8},
keywords = {reduced-crew operations, single pilot operations (SPO), teleconferencing},
location = {Santa Clara, California},
series = {HCI-Aero '14}
}

@inproceedings{10.1145/3575828.3575830,
author = {Huong, Nguyen Thanh and Hoang, Le Minh},
title = {Database Querying Optimization via Genetic Algorithm for Biomedical Research},
year = {2023},
isbn = {9781450397247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575828.3575830},
doi = {10.1145/3575828.3575830},
abstract = {Thanks to the skyscraping development of hardware and software technologies, the data solutions have become an urgent trend to deal with vast amount of data, especially in biomedical research, human genome and healthcare systems. The healthcare research has always demanded close association with biomedical data to produce personalized medicine and deliver suitable cure and treatments. Nevertheless, coping with huge amount of information from biomedical data requires bulky solutions. In the light of data science, the solution for this issue can change from a theoretical approach to a data-driven approach. Database stores a huge amount of information and particular sets of data can be accessed via queries which are written in specific interface language. In order to manage this amount of data, database optimization is implemented to maximize the speed and efficiency with data retrieval or reduce database system response time. Query optimization is one of the major functionalities in database management systems. The purpose of the query optimization is to determine the most efficient and effective way to execute a particular query by considering several query plans. In this article, genetic algorithm (GA) strategy is utilized for biomedical database systems to execute the query plan. Genetic algorithms are extensively using to solve constrained and unconstrained optimization problems. Based on three main types of rules of GA such as selection, crossover and mutation, the querying can be optimized for solving database problem.},
booktitle = {Proceedings of the 2022 7th International Conference on Systems, Control and Communications},
pages = {6–11},
numpages = {6},
keywords = {biomedical research, database, genetic algorithm, optimization, querying},
location = {Chongqing, China},
series = {ICSCC '22}
}

@proceedings{10.1145/3660043,
title = {ICIEAI '23: Proceedings of the 2023 International Conference on Information Education and Artificial Intelligence},
year = {2023},
isbn = {9798400716157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3544549,
title = {CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@inproceedings{10.1145/3594556.3594615,
author = {Chee, Cynthia Yi Min and Pal, Shantanu and Pan, Lei and Doss, Robin},
title = {An Analysis of Important Factors Affecting the Success of Blockchain Smart Contract Security Vulnerability Scanning Tools},
year = {2023},
isbn = {9798400701986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594556.3594615},
doi = {10.1145/3594556.3594615},
abstract = {Since the proposal and conception of smart contracts in blockchain-based platforms, the ever-growing use of smart contracts also comes with increased vulnerability exploits, leading to attacks causing significant losses. Various improvements and implementations have been made to strengthen smart contract security, including developing scanning tools to detect vulnerabilities in smart contracts, both open-source and commercial. These tools were met with varying degrees of success, with some thriving well while others have since stagnated. In this paper, we conducted case studies on six of these tools, three of which are highly regarded and still used today, while the other three have either limited success or have been deprecated. We identified three short-term (6 months or less) and five medium-term (6 to 24 months) factors affecting the tools’ successes. Based on our analysis, we outlined a framework with these factors for potential developers to consider for increasing the success of their projects.},
booktitle = {Proceedings of the 5th ACM International Symposium on Blockchain and Secure Critical Infrastructure},
pages = {105–113},
numpages = {9},
keywords = {Blockchain, Ethereum, Open source, Security., Smart contract, Vulnerability scanning tools},
location = {Melbourne, VIC, Australia},
series = {BSCI '23}
}

@proceedings{10.1145/3568562,
title = {SoICT '22: Proceedings of the 11th International Symposium on Information and Communication Technology},
year = {2022},
isbn = {9781450397254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hanoi, Vietnam}
}

@proceedings{10.1145/3628797,
title = {SOICT '23: Proceedings of the 12th International Symposium on Information and Communication Technology},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ho Chi Minh, Vietnam}
}

@inproceedings{10.1145/3658852.3659065,
author = {Cotton, Kelsey and De Vries, Katja and Tatar, K\i{}van\c{c}},
title = {Singing for the Missing: Bringing the Body Back to AI Voice and Speech Technologies},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658852.3659065},
doi = {10.1145/3658852.3659065},
abstract = {Technological advancements in deep learning for speech and voice have contributed to a recent expansion in applications for voice cloning, synthesis and generation. Invisibilised stakeholders in this expansion are numerous absent bodies, whose voices and voice data have been integral to the development and refinement of these speech technologies. This position paper probes current working practices for voice and speech in machine learning and AI, in which the bodies of voices are “invisibilised". We examine the facts and concerns about the voice-Body in applications of AI-voice technology. We do this through probing the wider connections between voice data and Schaefferian listening; speculating on the consequences of missing Bodies in AI-Voice; and by examining how vocalists and artists working with synthetic Bodies and AI-voices are ‘bringing the Body back’ in their own practices. We contribute with a series of considerations for how practitioners and researchers may help to ‘bring the Body back’ into AI-voice technologies.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {2},
numpages = {12},
keywords = {AI, STS, artificial intelligence, body, musical AI, voice},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/1873951.1874010,
author = {Xia, Jiazhi and He, Ying and Quynh, Dao P.T. and Chen, Xiaoming and Hoi, Steven C.H.},
title = {Modeling 3D facial expressions using geometry videos},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1874010},
doi = {10.1145/1873951.1874010},
abstract = {The significant advances in developing high-speed shape acquisition devices make it possible to capture the moving and deforming objects at video speeds. However, due to its complicated nature, it is technically challenging to effectively model and store the captured motion data. In this paper, we present a set of algorithms to construct geometry videos for 3D facial expressions, including hole filling, geodesic-based face segmentation, and expression-invariant parametrization. Our algorithms are efficient and robust, and can guarantee the exact correspondence of the salient features (eyes, mouth and nose). Geometry video naturally bridges the 3D motion data and 2D video, and provides a way to borrow the well-studied video processing techniques to motion data processing. With our proposed intra-frame prediction scheme based on H.264/AVC, we are able to compress the geometry videos into a very compact size while maintaining the video quality. Our experimental results on real-world datasets demonstrate that geometry video is effective for modeling the high-resolution 3D expression data.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {591–600},
numpages = {10},
keywords = {3D facial expression, H.264/AVC, feature correspondence, geometry video, motion data, motion data parametrization, video compression},
location = {Firenze, Italy},
series = {MM '10}
}

@inproceedings{10.1145/3184407.3184418,
author = {Ayala-Rivera, Vanessa and Kaczmarski, Maciej and Murphy, John and Darisa, Amarendra and Portillo-Dominguez, A. Omar},
title = {One Size Does Not Fit All: In-Test Workload Adaptation for Performance Testing of Enterprise Applications},
year = {2018},
isbn = {9781450350952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3184407.3184418},
doi = {10.1145/3184407.3184418},
abstract = {The identification of workload-dependent performance issues, as well as their root causes, is a time-consuming and complex process which typically requires several iterations of tests (as this type of issues can depend on the input workloads), and heavily relies on human expert knowledge. To improve this process, this paper presents an automated approach to dynamically adapt the workload (used by a performance testing tool) during the test runs. As a result, the performance issues of the tested application can be revealed more quickly; hence, identifying them with less effort and expertise. Our experimental evaluation has assessed the accuracy of the proposed approach and the time savings that it brings to testers. The results have demonstrated the benefits of the approach by achieving a significant decrease in the time invested in performance testing (without compromising the accuracy of the test results), while introducing a low overhead in the testing environment.},
booktitle = {Proceedings of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {211–222},
numpages = {12},
keywords = {analysis, automation, performance, testing, workload},
location = {Berlin, Germany},
series = {ICPE '18}
}

@proceedings{10.1145/3643479,
title = {AIQAM '24: Proceedings of the 1st ACM Workshop on AI-Powered Q&amp;A Systems for Multimedia},
year = {2024},
isbn = {9798400705472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Phuket, Thailand}
}

@article{10.1613/jair.1.13318,
author = {Surynek, Pavel and Stern, Roni and Boyarski, Eli and Felner, Ariel},
title = {Migrating Techniques from Search-based Multi-Agent Path Finding Solvers to SAT-based Approach},
year = {2022},
issue_date = {May 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {73},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13318},
doi = {10.1613/jair.1.13318},
abstract = {In the multi-agent path finding problem (MAPF) we are given a set of agents each with respective start and goal positions. The task is to find paths for all agents while avoiding collisions, aiming to minimize a given objective function. Many MAPF solvers were introduced in the past decade for optimizing two specific objective functions: sum-of-costs and makespan. Two prominent categories of solvers can be distinguished: search-based solvers and compilation-based solvers. Search-based solvers were developed and tested for the sum-of-costs objective, while the most prominent compilation-based solvers that are built around Boolean satisfiability (SAT) were designed for the makespan objective. Very little is known on the performance and relevance of solvers from the compilation-based approach on the sum-of-costs objective. In this paper, we start to close the gap between these cost functions in the compilation-based approach. Our main contribution is a new SAT-based MAPF solver called MDD-SAT, that is directly aimed to optimally solve the MAPF problem under the sum-of-costs objective function. Using both a lower bound on the sum-of-costs and an upper bound on the makespan, MDD-SAT is able to generate a reasonable number of Boolean variables in our SAT encoding. We then further improve the encoding by borrowing ideas from ICTS, a search-based solver. In addition, we show that concepts applicable in search-based solvers like ICTS and ICBS are applicable in the SAT-based approach as well. Specifically, we integrate independence detection, a generic technique for decomposing an MAPF instance into independent subproblems, into our SAT-based approach, and we design a relaxation of our optimal SAT-based solver that results in a bounded suboptimal SAT-based solver. Experimental evaluation on several domains shows that there are many scenarios where our SAT-based methods outperform state-of-the-art sum-of-costs search-based solvers, such as variants of the ICTS and ICBS algorithms.},
journal = {J. Artif. Int. Res.},
month = {may},
numpages = {66},
keywords = {problem solving, satisfiability, constraint satisfaction, planning}
}

@inproceedings{10.1145/3053600.3053614,
author = {Kaczmarski, Maciej and Perry, Philip and Murphy, John and Portillo-Dominguez, A. Omar},
title = {In-Test Adaptation of Workload in Enterprise Application Performance Testing},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053614},
doi = {10.1145/3053600.3053614},
abstract = {Performance testing is used to assess if an enterprise application can fulfil its expected Service Level Agreements. However, since some performance issues depend on the input workloads, it is common to use time-consuming and complex iterative test methods, which heavily rely on human expertise. This paper presents an automated approach to dynamically adapt the workload so that issues (e.g. bottlenecks) can be identified more quickly as well as with less effort and expertise. We present promising results from an initial validation prototype indicating an 18-fold decrease in the test time without compromising the accuracy of the test results, while only introducing a marginal overhead in the system.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {69–72},
numpages = {4},
keywords = {analysis, engineering, performance, testing},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1145/3643650.3658605,
author = {Lomotey, Richard K. and Kumi, Sandra and Ray, Madhurima and Deters, Ralph},
title = {Synthetic Data Digital Twins and Data Trusts Control for Privacy in Health Data Sharing},
year = {2024},
isbn = {9798400705557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643650.3658605},
doi = {10.1145/3643650.3658605},
abstract = {Health data sharing is very valuable for medical research since it has the propensity to improve diagnostics, policy, medication, and so on. At the same time, sharing health data needs to be done without compromising the privacy of patients and stakeholders. However, recent advances in AI/ML and sophisticated analytics have proven to introduce biases that can easily identify patients based on their healthcare data, which violates privacy. In this work, we sort to address this major issue by exploring two emerging topics that are gaining attention from industry, academia, and governments, i.e., digital twins and data trusts. First, we proposed the use of digital twins (DTs) to generate synthetic records of patient's heart rate data. DTs are virtual replicas of the actual data and were created using two synthetic data generative models - Gaussian Copula (GC) and Tabular Variational Autoencoder (TVAE). The GC and TVAE achieved a maximum data quality score of 88\% and 96\% respectively. Next, we posit that the DTs should be shared with a data trusts layer. Data trusts are fiduciary frameworks that govern multi-party data sharing. The data trusts enforce access controls (based on metrics such as location, role-based, and policy-based) to the synthetic health data and reports to the data subject. The preliminary evaluations of the work show that merging the two techniques (i.e., synthetic data digital twins and data trusts) enforces better privacy for health data access. The synthetic data ensures more anonymization while the data trusts provide easy auditing, tracking, and efficient reporting to the patient or data subject. The paper also detailed the architectural design of the data trusts and evaluated the efficiency of the access control techniques.},
booktitle = {Proceedings of the 2024 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems},
pages = {1–10},
numpages = {10},
keywords = {artificial intelligence, data trusts, digital twins, machine learning, middleware, privacy, synthetic health data},
location = {Porto, Portugal},
series = {SaT-CPS '24}
}

@article{10.1145/3372390,
author = {Heldens, Stijn and Hijma, Pieter and Werkhoven, Ben Van and Maassen, Jason and Belloum, Adam S. Z. and Van Nieuwpoort, Rob V.},
title = {The Landscape of Exascale Research: A Data-Driven Literature Analysis},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3372390},
doi = {10.1145/3372390},
abstract = {The next generation of supercomputers will break the exascale barrier. Soon we will have systems capable of at least one quintillion (billion billion) floating-point operations per second (1018 FLOPS). Tremendous amounts of work have been invested into identifying and overcoming the challenges of the exascale era. In this work, we present an overview of these efforts and provide insight into the important trends, developments, and exciting research opportunities in exascale computing. We use a three-stage approach in which we (1) discuss various exascale landmark studies, (2) use data-driven techniques to analyze the large collection of related literature, and (3) discuss eight research areas in depth based on influential articles. Overall, we observe that great advancements have been made in tackling the two primary exascale challenges: energy efficiency and fault tolerance. However, as we look forward, we still foresee two major concerns: the lack of suitable programming tools and the growing gap between processor performance and data bandwidth (i.e., memory, storage, networks). Although we will certainly reach exascale soon, without additional research, these issues could potentially limit the applicability of exascale computing.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {23},
numpages = {43},
keywords = {Exascale computing, data-driven analysis, extreme-scale computing, high-performance computing, literature review}
}


@article{10.1145/3028687.3043967,
author = {Narayanan, Arvind and Miller, Andrew and Han, Song and Bailis, Peter},
title = {Research for Practice: Cryptocurrencies, Blockchains, and Smart Contracts; Hardware for Deep Learning: Expert-curated Guides to the Best of CS Research},
year = {2016},
issue_date = {November-December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1542-7730},
url = {https://doi.org/10.1145/3028687.3043967},
doi = {10.1145/3028687.3043967},
abstract = {First, Arvind Narayanan and Andrew Miller, co-authors of the increasingly popular open-access Princeton Bitcoin textbook, provide an overview of ongoing research in cryptocurrencies. Second, Song Han provides an overview of hardware trends related to another long-studied academic problem that has recently seen an explosion in popularity: deep learning.},
journal = {Queue},
month = {dec},
pages = {43–55},
numpages = {13}
}

@proceedings{10.1145/3548608,
title = {ICCIR '22: Proceedings of the 2022 2nd International Conference on Control and Intelligent Robotics},
year = {2022},
isbn = {9781450397179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanjing, China}
}

@article{10.1145/3606702,
author = {Adamou, Alessandro and Picca, Davide and Hou, Yumeng and Loreto Granados-Garc\'{\i}a, Paula},
title = {The Facets of Intangible Heritage in Southern Chinese Martial Arts: Applying a Knowledge-driven Cultural Contact Detection Approach},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3606702},
doi = {10.1145/3606702},
abstract = {Investigating the intangible nature of a cultural domain can take multiple forms, addressing, for example, the aesthetic, epistemic, and social dimensions of its phenomenology. The context of Southern Chinese martial arts is of particular significance, as it carries immaterial components of all these aspects: The technical and stylistic framework of a martial art system; the imagery associated to movements; and the transmission of knowledge orally, practically, or through influence, are but examples of intangible characteristics that can and should be captured, not unlike cultural artifacts. The latter case—the one of formalizing cultural influence through its various forms of evidenceis emblematic as well as largely untrodden ground. A previous attempt at detecting cultural influence computationally was made in the context of Roman archaeology, though the binding of that early effort with the domain model was tight; also, there has hardly been any prior dedicated effort to model the martial arts domain through ontologies. In this article, we present the realization of the full cycle of a computational approach to investigating cultural contact in Southern Chinese martial arts. The entire approach is predicated upon the usage of standards and techniques of the Semantic Web and formal knowledge. Starting from a modular domain ontology, which models martial arts independently of the goal of capturing cultural influence, we perform knowledge extraction from archival material from the Hong Kong Martial Arts Living Archive and generate a dataset of the results modeled after said ontology. Then, we combine the resulting knowledge base with a rule model that represents ways to infer knowledge of potential contact between cultures based on the evidence present in the knowledge base. The results offer an insight into how an inference-based computational model can be applied to detect interesting facts even in the as-yet underexplored domain of intangible cultural heritage. The implemented workflow shows that the full-cycle employment of semantic technologies can offer the ground truth required for largely different approaches, such as statistical and machine learning ones, to operate.},
journal = {J. Comput. Cult. Herit.},
month = {aug},
articleno = {63},
numpages = {27},
keywords = {Intangible cultural heritage, digitization, Semantic Web, embodied knowledge, knowledge representation, ontologies, inferencing}
}

@inproceedings{10.1145/2808719.2808735,
author = {Deodhar, Suruchi and Chen, Jiangzhuo and Wilson, Mandy and Bisset, Keith and Barrett, Chris and Marathe, Madhav},
title = {EpiCaster: an integrated web application for situation assessment and forecasting of global epidemics},
year = {2015},
isbn = {9781450338530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808719.2808735},
doi = {10.1145/2808719.2808735},
abstract = {Public health decision makers need access to high resolution situation assessment tools for understanding the extent of various epidemics in different regions of the world. In addition, they need insights into the future course of epidemics by way of forecasts. Such forecasts are essential for planning the allocation of limited resources and for implementing several policy-level and behavioral intervention strategies. The need for such forecasting systems became evident in the wake of the recent Ebola outbreak in West Africa.We have developed EpiCaster, an integrated Web application for situation assessment and forecasting of various epidemics, such as Flu and Ebola, that are prevalent in different regions of the world. Using EpiCaster, users can assess the magnitude and severity of different epidemics at highly resolved spatio-temporal levels. EpiCaster provides time-varying heat maps and graphical plots to view trends in the disease dynamics. EpiCaster also allows users to visualize data gathered through surveillance mechanisms, such as Google Flu Trends (GFT) and the World Health Organization (WHO). The forecasts provided by EpiCaster are generated using different epidemiological models, and the users can select the models through the interface to filter the corresponding forecasts. EpiCaster also allows the users to study epidemic propagation in the presence of a number of intervention strategies specific to certain diseases.Here we describe the modeling techniques, methodologies and computational infrastructure that EpiCaster relies on to support large-scale predictive analytics for situation assessment and forecasting of global epidemics.},
booktitle = {Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {156–165},
numpages = {10},
location = {Atlanta, Georgia},
series = {BCB '15}
}

@proceedings{10.1145/3634737,
title = {ASIA CCS '24: Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM AsiaCCS 2024, the 19th ACM Asia Conference on Computer and Communications Security. AsiaCCS 2024 takes place in Singapore from 1 July to 5 July.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3629264.3629283,
author = {Qu, Jiayi and Li, Hehua},
title = {Key Applications and Optimization Strategies of Data Mining in Bank's Loyalty Rewards Mall},
year = {2023},
isbn = {9798400700576},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629264.3629283},
doi = {10.1145/3629264.3629283},
abstract = {The bank's loyalty rewards mall has drawn substantial attention as a key strategy to entice and promote user engagement in today's competitive financial sector. However, given consumers' increasingly varied purchasing habits and tastes, banks face a significant challenge in raising customer happiness and maximizing revenues. Banks have used big data technologies to extract crucial data from enormous databases, better understand consumer expectations, and refine their loyalty rewards mall's operations and marketing tactics to overcome these obstacles. In this project, order data from a certain bank's loyalty rewards mall will be cleaned, preprocessed, and visualized using data analytic tools. It aims to gain a deeper understanding of customer demands and preferences by analyzing product sales and the mall's profitability, increasing customer satisfaction and loyalty.},
booktitle = {Proceedings of the 2023 7th International Conference on Computing and Data Analysis},
pages = {21–27},
numpages = {7},
keywords = {bank's loyalty rewards mall, data analysis, data visualization},
location = {Guiyang, China},
series = {ICCDA '23}
}

@article{10.1145/3642979.3643006,
author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
title = {Large Search Model: Redefining Search Stack in the Era of LLMs},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3642979.3643006},
doi = {10.1145/3642979.3643006},
abstract = {Modern search engines are built on a stack of different components, including query understanding, retrieval, multi-stage ranking, and question answering, among others. These components are often optimized and deployed independently. In this paper, we introduce a novel conceptual framework called large search model, which redefines the conventional search stack by unifying search tasks with one large language model (LLM). All tasks are formulated as autoregressive text generation problems, allowing for the customization of tasks through the use of natural language prompts. This proposed framework capitalizes on the strong language understanding and reasoning capabilities of LLMs, offering the potential to enhance search result quality while simultaneously simplifying the existing cumbersome search stack. To substantiate the feasibility of this framework, we present a series of proof-of-concept experiments and discuss the potential challenges associated with implementing this approach within real-world search systems.},
journal = {SIGIR Forum},
month = {jan},
articleno = {23},
numpages = {16}
}

@inproceedings{10.1109/ICSE48619.2023.00036,
author = {Zheng, Zibin and Zhang, Neng and Su, Jianzhong and Zhong, Zhijie and Ye, Mingxi and Chen, Jiachi},
title = {Turn the Rudder: A Beacon of Reentrancy Detection for Smart Contracts on Ethereum},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00036},
doi = {10.1109/ICSE48619.2023.00036},
abstract = {Smart contracts are programs deployed on a blockchain and are immutable once deployed. Reentrancy, one of the most important vulnerabilities in smart contracts, has caused millions of dollars in financial loss. Many reentrancy detection approaches have been proposed. It is necessary to investigate the performance of these approaches to provide useful guidelines for their application. In this work, we conduct a large-scale empirical study on the capability of five well-known or recent reentrancy detection tools such as Mythril and Sailfish. We collect 230,548 verified smart contracts from Etherscan and use detection tools to analyze 139,424 contracts after deduplication, which results in 21,212 contracts with reentrancy issues. Then, we manually examine the defective functions located by the tools in the contracts. From the examination results, we obtain 34 true positive contracts with reentrancy and 21,178 false positive contracts without reentrancy. We also analyze the causes of the true and false positives. Finally, we evaluate the tools based on the two kinds of contracts. The results show that more than 99.8\% of the reentrant contracts detected by the tools are false positives with eight types of causes, and the tools can only detect the reentrancy issues caused by call.value(), 58.8\% of which can be revealed by the Ethereum's official IDE, Remix. Furthermore, we collect real-world reentrancy attacks reported in the past two years and find that the tools fail to find any issues in the corresponding contracts. Based on the findings, existing works on reentrancy detection appear to have very limited capability, and researchers should turn the rudder to discover and detect new reentrancy patterns except those related to call.value().},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {295–306},
numpages = {12},
keywords = {smart contract, reentrancy, empirical study},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.1145/3605098,
title = {SAC '24: Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the Organizing Committee, I extend a warm welcome to you at the 39th Annual ACM Symposium on Applied Computing (SAC 2024), taking place in \'{A}vila, Spain, and hosted by the University of Salamanca. For more than three decades, this international forum has been dedicated to computer scientists, engineers, and practitioners, providing a platform for presenting their research findings and results in various areas of applied computing. The organizing committee sincerely appreciates your participation in this exciting international event, and we hope that the conference proves interesting and beneficial for all attendees.},
location = {Avila, Spain}
}

@inproceedings{10.1145/3492323.3495577,
author = {Wang, Gang and Nixon, Mark},
title = {SoK: tokenization on blockchain},
year = {2022},
isbn = {9781450391634},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492323.3495577},
doi = {10.1145/3492323.3495577},
abstract = {Blockchain, a potentially disruptive technology, advances many different applications, e.g., crypto-currencies, supply chains, and the Internet of Things. Under the hood of blockchain, it is required to handle different kinds of digital assets and data. The next-generation blockchain ecosystem is expected to consist of numerous applications, and each application may have a distinct representation of digital assets. However, digital assets cannot be directly recorded on the blockchain, and a tokenization process is required to format these assets. Tokenization on blockchain will inevitably require a certain level of proper standards to enrich advanced functionalities and enhance interoperable capabilities for future applications. However, due to specific features of digital assets, it is hard to obtain a standard token form to represent all kinds of assets. For example, when considering fungibility, some assets are divisible and identical, commonly referred to as fungible assets. In contrast, others that are not fungible are widely referred to as non-fungible assets. When tokenizing these assets, we are required to follow different tokenization processes. The way to effectively tokenize assets is thus essential and expecting to confront various unprecedented challenges. This paper provides a systematic and comprehensive study of the current progress of tokenization on blockchain. First, we explore general principles and practical schemes to tokenize digital assets for blockchain and classify digitized tokens into three categories: fungible, non-fungible, and semi-fungible. We then focus on discussing the well-known Ethereum standards on non-fungible tokens. Finally, we discuss several critical challenges and some potential research directions to advance the research on exploring the tokenization process on the blockchain. To the best of our knowledge, this is the first systematic study for tokenization on blockchain.},
booktitle = {Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion},
articleno = {11},
numpages = {9},
keywords = {blockchain, fungible token, non-fungible token (NFT), tokenization},
location = {Leicester, United Kingdom},
series = {UCC '21}
}

@inproceedings{10.1109/MSR.2017.59,
author = {Wan, Zhiyuan and Lo, David and Xia, Xin and Cai, Liang},
title = {Bug characteristics in blockchain systems: a large-scale empirical study},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.59},
doi = {10.1109/MSR.2017.59},
abstract = {Bugs severely hurt blockchain system dependability. A thorough understanding of blockchain bug characteristics is required to design effective tools for preventing, detecting and mitigating bugs. We perform an empirical study on bug characteristics in eight representative open source blockchain systems. First, we manually examine 1,108 bug reports to understand the nature of the reported bugs. Second, we leverage card sorting to label the bug reports, and obtain ten bug categories in blockchain systems. We further investigate the frequency distribution of bug categories across projects and programming languages. Finally, we study the relationship between bug categories and bug fixing time. The findings include: (1) semantic bugs are the dominant runtime bug category; (2) frequency distributions of bug types show similar trends across different projects and programming languages; (3) security bugs take the longest median time to be fixed; (4) 35.71\% performance bugs are fixed in more than one year; performance bugs take the longest average time to be fixed.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {413–424},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@proceedings{10.1145/3603765,
title = {ICISDM '23: Proceedings of the 2023 7th International Conference on Information System and Data Mining},
year = {2023},
isbn = {9798400700637},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Atlanta, USA}
}

@inproceedings{10.1145/223982.224433,
author = {Dao, Binh Vien and Duato, Jose and Yalamanchili, Sudhakar},
title = {Configurable flow control mechanisms for fault-tolerant routing},
year = {1995},
isbn = {0897916980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223982.224433},
doi = {10.1145/223982.224433},
abstract = {Fault-tolerant routing protocols in modern interconnection networks rely heavily on the network flow control mechanisms used. Optimistic flow control mechanisms such as wormhole routing (WR) realize very good performance, but are prone to deadlock in the presence of faults. Conservative flow control mechanisms such as pipelined circuit switching (PCS) insures existence of a path to the destination prior to message transmission, but incurs increased overhead. Existing fault-tolerant routing protocols are designed with one or the other, and must accommodate their associated constraints. This paper proposes the use of configurable flow control mechanisms. Routing protocols can then be designed such that in the vicinity of faults, protocols use a more conservative flow control mechanism, while the majority of messages that traverse fault-free portions of the network utilize a WR like flow control to maximize performance. Such protocols are referred to as two-phase protocols, where routing decisions are provided some control over the operation of the virtual channels. This ability provides new avenues for optimizing message passing performance in the presence of faults. A fully adaptive two-phase protocol is proposed and compared via simulation to those based on WR and PCS. The architecture of a network router supporting configurable flow control is described, and the paper concludes with avenues for future research.},
booktitle = {Proceedings of the 22nd Annual International Symposium on Computer Architecture},
pages = {220–229},
numpages = {10},
location = {S. Margherita Ligure, Italy},
series = {ISCA '95}
}

@article{10.1145/225830.224433,
author = {Dao, Binh Vien and Duato, Jose and Yalamanchili, Sudhakar},
title = {Configurable flow control mechanisms for fault-tolerant routing},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/225830.224433},
doi = {10.1145/225830.224433},
abstract = {Fault-tolerant routing protocols in modern interconnection networks rely heavily on the network flow control mechanisms used. Optimistic flow control mechanisms such as wormhole routing (WR) realize very good performance, but are prone to deadlock in the presence of faults. Conservative flow control mechanisms such as pipelined circuit switching (PCS) insures existence of a path to the destination prior to message transmission, but incurs increased overhead. Existing fault-tolerant routing protocols are designed with one or the other, and must accommodate their associated constraints. This paper proposes the use of configurable flow control mechanisms. Routing protocols can then be designed such that in the vicinity of faults, protocols use a more conservative flow control mechanism, while the majority of messages that traverse fault-free portions of the network utilize a WR like flow control to maximize performance. Such protocols are referred to as two-phase protocols, where routing decisions are provided some control over the operation of the virtual channels. This ability provides new avenues for optimizing message passing performance in the presence of faults. A fully adaptive two-phase protocol is proposed and compared via simulation to those based on WR and PCS. The architecture of a network router supporting configurable flow control is described, and the paper concludes with avenues for future research.},
journal = {SIGARCH Comput. Archit. News},
month = {may},
pages = {220–229},
numpages = {10}
}

@inproceedings{10.1145/3468978.3468993,
author = {Thi Pham, Hue and Kieu Thi Nguyen, Tien},
title = {Research and Application of "Digital Transformation" in Teaching "History of Vietnam Wars in Modern Times" at FPT Education},
year = {2021},
isbn = {9781450390354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468978.3468993},
doi = {10.1145/3468978.3468993},
abstract = {In the time of global integration, the Vietnam's education sector in general and Vietnam's history in particular has to be in line with the global trend which is to take advantage of the achievements resulted from the Fourth Industrial Revolution (Industry 4.0) to increase excitement for learners. Therefore, studying and applying the achievements of the "digital transformation revolution" into teaching the subject "History of Vietnam wars in modern times" is an indispensable task. For this reason, the article proposed a number of strategies to apply achievements of the "digital transformation revolution" in this subject in order to increase the learners’ interest and also to follow trend of the Industry 4.0.},
booktitle = {2021 3rd International Conference on Modern Educational Technology},
pages = {89–92},
numpages = {4},
keywords = {Digital transformation, FPT Education, Vietnam, teaching, war history},
location = {Jakarta, Indonesia},
series = {ICMET 2021}
}

@proceedings{10.1145/3660512,
title = {SCID '24: Proceedings of the 1st Workshop on Security-Centric Strategies for Combating Information Disorder},
year = {2024},
isbn = {9798400706509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3605390,
title = {CHItaly '23: Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter},
year = {2023},
isbn = {9798400708060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Torino, Italy}
}

@article{10.1145/3656343,
author = {Li, Mingzhe and Wang, Wei and Zhang, Jin},
title = {Towards Efficient and Deposit-Free Blockchain-Based Spatial Crowdsourcing},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3656343},
doi = {10.1145/3656343},
abstract = {Spatial crowdsourcing leverages the widespread use of mobile devices to outsource tasks to a crowd of users based on their geographical location. Despite its growing popularity, current crowdsourcing systems often suffer from a lack of transparency, centralization, and other security issues. Blockchain technology has revolutionized this sector with its potential for decentralization, security, and transparency. However, existing blockchain-based crowdsourcing systems often overlook efficient task assignment mechanisms and expose users to potential losses due to the obligatory deposit payments to smart contracts, which might be vulnerable or untrustworthy.This article proposes EDF-Crowd, an Efficient and Deposit-Free blockchain-based spatial crowdsoucing framework, to address these challenges. EDF-Crowd introduces an efficient, customizable task assignment mechanism based on smart contracts, operating periodically and batch-wise. We also design a fair compensation mechanism to compensate users for the extra overhead caused by invoking certain smart contracts. More importantly, we propose a series of linkage protocols. By linking users’ back-and-forth actions, EDF-Crowd can regulate user behavior without requiring users to deposit. The versatility of EDF-Crowd also allows its application to generic crowdsourcing systems with minimal modifications. We implement EDF-Crowd based on the EOS blockchain. Extensive evaluations show that EDF-Crowd achieves high task assignment efficiency and low cost.},
journal = {ACM Trans. Sen. Netw.},
month = {may},
articleno = {73},
numpages = {22},
keywords = {Blockchain, crowdsourcing, spatial crowdsourcing, task assignment, smart contract, deposit}
}

@inproceedings{10.1145/508171.508177,
author = {Zhang, Yongguang and Vin, Harrick and Alvisi, Lorenzo and Lee, Wenke and Dao, Son K.},
title = {Heterogeneous networking: a new survivability paradigm},
year = {2001},
isbn = {1581134576},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/508171.508177},
doi = {10.1145/508171.508177},
abstract = {We believe that a network, to be survivable, must be heterogeneous. Just like a species that draws on a small gene pool can succumb to a single environmental threat, so a homogeneous network is vulnerable to a malicious attack that exploits a single weakness common to all of its components. In contrast, in a network in which each critical functionality is provided by a diverse set of protocols and implementations, attacks that focus on a weakness of one such protocol or implementation will not be able to bring down the entire network, even though all elements are not be bulletproof and even if some of components are compromised.Following this survivability through heterogeneity philosophy, we propose a new survivability paradigm, called heterogeneous networking, for improving a network's defense capabilities. Rather than following the current trend of converging towards single solutions to provide the desired functionality at every element of the network architecture, this methodology calls for systematically increasing the network's heterogeneity without sacrificing its interoperability.},
booktitle = {Proceedings of the 2001 Workshop on New Security Paradigms},
pages = {33–39},
numpages = {7},
keywords = {diversity, heterogeneity, network security, survivability},
location = {Cloudcroft, New Mexico},
series = {NSPW '01}
}

@article{10.1109/TCBB.2021.3063284,
author = {Sun, Yesen and Ou-Yang, Le and Dai, Dao-Qing},
title = {WMLRR: A Weighted Multi-View Low Rank Representation to Identify Cancer Subtypes From Multiple Types of Omics Data},
year = {2021},
issue_date = {Nov.-Dec. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3063284},
doi = {10.1109/TCBB.2021.3063284},
abstract = {The identification of cancer subtypes is of great importance for understanding the heterogeneity of tumors and providing patients with more accurate diagnoses and treatments. However, it is still a challenge to effectively integrate multiple omics data to establish cancer subtypes. In this paper, we propose an unsupervised integration method, named weighted multi-view low rank representation (WMLRR), to identify cancer subtypes from multiple types of omics data. Given a group of patients described by multiple omics data matrices, we first learn a unified affinity matrix which encodes the similarities among patients by exploring the sparsity-consistent low-rank representations from the joint decompositions of multiple omics data matrices. Unlike existing subtype identification methods that treat each omics data matrix equally, we assign a weight to each omics data matrix and learn these weights automatically through the optimization process. Finally, we apply spectral clustering on the learned affinity matrix to identify cancer subtypes. Experiment results show that the survival times between our identified cancer subtypes are significantly different, and our predicted survivals are more accurate than other state-of-the-art methods. In addition, some clinical analyses of the diseases also demonstrate the effectiveness of our method in identifying molecular subtypes with biological significance and clinical relevance.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {2891–2897},
numpages = {7}
}

@inproceedings{10.1145/3581783.3611770,
author = {Li, Chaoyang and Zhang, Rui-Xiao and Huang, Tianchi and Jia, Lianchen and Sun, Lifeng},
title = {Concerto: Client-server Orchestration for Real-Time Video Analytics},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611770},
doi = {10.1145/3581783.3611770},
abstract = {The delay to obtain analysis results is an important metric in video analytics. Previous work has focused on reducing frame transmission and inference delay to optimize total delay. However, network fluctuations can cause frames to arrive at the backend simultaneously, leading to backend queuing delays. To address this issue, we propose Concerto, a joint front-and backend video analytics pipeline that optimizes both network transmission and backend queuing delays. The backend controls the frame queue, accelerating or skipping inference as needed to mitigate backend queuing delay. The frontend considers both delays when configuring frames to send, resulting in better total delay. Experiments show that Concerto significantly reduces backend queuing delay with minimal loss of accuracy.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9215–9223},
numpages = {9},
keywords = {edge offloading, neural networks, video analytics},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3611643.3613083,
author = {Happe, Andreas and Cito, J\"{u}rgen},
title = {Getting pwn’d by AI: Penetration Testing with Large Language Models},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613083},
doi = {10.1145/3611643.3613083},
abstract = {The field of software security testing, more specifically penetration testing, requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential use of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of AI sparring partners.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2082–2086},
numpages = {5},
keywords = {large language models, penetration testing, security testing},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@proceedings{10.1145/3569219,
title = {Academic Mindtrek '22: Proceedings of the 25th International Academic Mindtrek Conference},
year = {2022},
isbn = {9781450399555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tampere, Finland}
}

@article{10.1145/3660347,
author = {Hussain, Walayat and Gao, Honghao and Karim, Rafiul and Saddik, Abdulmotaleb El},
title = {Seventeen Years of the ACM Transactions on Multimedia Computing, Communications and Applications: A Bibliometric Overview},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3660347},
doi = {10.1145/3660347},
abstract = {ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) has been dedicated to advancing multimedia research, fostering discoveries, innovations, and practical applications since 2005. The journal consistently publishes top-notch, original research in emerging fields through open submissions, calls for papers, special issues, rigorous review processes, and diverse research topics. This study aims to delve into an extensive bibliometric analysis of the journal, utilising various bibliometric indicators. The paper seeks to unveil the latent implications within the journal’s scholarly landscape from 2005 to 2022. The data primarily draws from the Web of Science (WoS) Core Collection database. The analysis encompasses diverse viewpoints, including yearly publication rates and citations, identifying highly cited papers, and assessing the most prolific authors, institutions, and countries. The paper employs VOSviewer-generated graphical maps, effectively illustrating networks of co-citations, keyword co-occurrences, and institutional and national bibliographic couplings. Furthermore, the study conducts a comprehensive global and temporal examination of co-occurrences of the author’s keywords. This investigation reveals the emergence of numerous novel keywords over the past decades.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {apr},
keywords = {Bibliometric; Transactions on Multimedia Computing, Communication and Applications; Citation Analysis; Web of Science; VOS Viewer.}
}

@article{10.1145/3660773,
author = {Hossain, Soneya Binta and Jiang, Nan and Zhou, Qiang and Li, Xiaopeng and Chiang, Wen-Hao and Lyu, Yingjun and Nguyen, Hoan and Tripp, Omer},
title = {A Deep Dive into Large Language Models for Automated Bug Localization and Repair},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660773},
doi = {10.1145/3660773},
abstract = {Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
including automated program repair (APR). In this study, we take a deep dive into automated bug localization
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
and repair utilizing LLMs. In contrast to many deep learning-based APR methods that assume known bug
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
fixing. This methodological separation of bug localization and fixing using different LLMs enables effective
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
integration of diverse contextual information and improved incorporation of inductive biases. We introduce
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Toggle: Token-Granulated Bug Localization and Repair, a comprehensive program repair framework
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
that integrates a bug localization model, an adjustment model to address tokenizer inconsistencies, and a
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
bug-fixing model. Toggle takes a buggy function as input and generates a complete corrected function. We
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
investigate various styles of prompting to the bug fixing model to identify the most effective prompts that
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
better utilize the inductive bias and significantly outperform others. Toggle achieves the new state-of-the-art
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(SOTA) performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
performance on several other widely-used APR datasets, including Defects4J. In the Defects4J benchmark, our
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
approach consistently ranks above other methods, achieving superior results in the Top-10, Top-30, Top-50,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
and Top-100 metrics. Besides examining Toggle’s generalizability to unseen data, evaluating the effectiveness
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
of various prompts, we also investigate the impact of additional contextual information such as buggy lines
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
and code comments on bug localization, and explore the importance of the adjustment model. Our extensive
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
experiments offer valuable insights and answers to critical research questions.},
journal = {Proc. ACM Softw. Eng.},
month = {jul},
articleno = {66},
numpages = {23},
keywords = {Automated Bug Localization and Reapir, Large Language Models}
}

@inproceedings{10.1145/3498891.3498903,
author = {Gallagher, Kevin and Torres-Arias, Santiago and Memon, Nasir and Feldman, Jessica},
title = {COLBAC: Shifting Cybersecurity from Hierarchical to Horizontal Designs},
year = {2022},
isbn = {9781450385732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498891.3498903},
doi = {10.1145/3498891.3498903},
abstract = {Cybersecurity suffers from an oversaturation of centralized, hierarchical systems and a lack of exploration in the area of horizontal security, or security techniques and technologies which utilize democratic participation for security decision-making. Because of this, many horizontally governed organizations such as activist groups, worker cooperatives, trade unions, not-for-profit associations, and others are not represented in current cybersecurity solutions, and are forced to adopt hierarchical solutions to cybersecurity problems. This causes power dynamic mismatches that lead to cybersecurity and organizational operations failures. In this work we introduce COLBAC, a collective based access control system aimed at addressing this lack. COLBAC uses democratically authorized capability tokens to express access control policies. It allows for a flexible and dynamic degree of horizontality to meet the needs of different horizontally governed organizations. After introducing COLBAC, we finish with a discussion on future work needed to realize more horizontal security techniques, tools, and technologies.},
booktitle = {Proceedings of the 2021 New Security Paradigms Workshop},
pages = {13–27},
numpages = {15},
keywords = {access control, activism, authorization, democracy, distributed systems, horizontality, participation, participatory design, security},
location = {Virtual Event, USA},
series = {NSPW '21}
}

@inproceedings{10.1145/3589462.3589494,
author = {Youssfi Nouira, Ameni and Bouchakwa, Mariam and Jamoussi, Yassine},
title = {Bitcoin Price Prediction Considering Sentiment Analysis on Twitter and Google News},
year = {2023},
isbn = {9798400707445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589462.3589494},
doi = {10.1145/3589462.3589494},
abstract = {Cryptocurrencies are digital currencies that operate on the blockchain, which is the technology that offers security and decentralization. The principal characteristic of cryptocurrencies is that they are not generally issued by a central authority. Many factors can influence the volatility of prices. This paper enables to drive insights into the behavior of markets through the application of sentiment analysis of Tweets, Google news and machine learning techniques for the challenging task of cryptocurrency price prediction. Most of the studies have focused exclusively on the sentiment analysis of tweets. In this work, we propose the use of common machine learning tools and available Google News data for predicting the price of crypto. We present the results of the Long Short-Term Memory (LSTM) model using Tweets and Google News data.},
booktitle = {Proceedings of the 27th International Database Engineered Applications Symposium},
pages = {71–78},
numpages = {8},
keywords = {Google news data, LSTM, Sentiment Analysis, Tweets, machine learning, price bitcoin prediction},
location = {Heraklion, Crete, Greece},
series = {IDEAS '23}
}

@inproceedings{10.1145/3624062.3624196,
author = {Shekofteh, S.-Kazem and Alles, Christian and Fr\"{o}ning, Holger},
title = {Reducing Memory Requirements for the IPU using Butterfly Factorizations},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624196},
doi = {10.1145/3624062.3624196},
abstract = {High Performance Computing (HPC) benefits from different improvements during last decades, specially in terms of hardware platforms to provide more processing power while maintaining the power consumption at a reasonable level. The Intelligence Processing Unit (IPU) is a new type of massively parallel processor, designed to speedup parallel computations with huge number of processing cores and on-chip memory components connected with high-speed fabrics. IPUs mainly target machine learning applications, however, due to the architectural differences between GPUs and IPUs, especially significantly less memory capacity on an IPU, methods for reducing model size by sparsification have to be considered. Butterfly factorizations are well-known replacements for fully-connected and convolutional layers. In this paper, we examine how butterfly structures can be implemented on an IPU and study their behavior and performance compared to a GPU. Experimental results indicate that these methods can provide 98.5\% compression ratio to decrease the immense need for memory, the IPU implementation can benefit from 1.3x and 1.6x performance improvement for butterfly and pixelated butterfly, respectively. We also reach to 1.62x training time speedup on a real-word dataset such as CIFAR10.},
booktitle = {Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1255–1263},
numpages = {9},
keywords = {Intelligence Processing Units, Massively Parallel Processing, Sparse Data Structure},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@proceedings{10.1145/3652583,
title = {ICMR '24: Proceedings of the 2024 International Conference on Multimedia Retrieval},
year = {2024},
isbn = {9798400706196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to present the 2024 edition of the ACM International Conference on Multimedia Retrieval, ACM ICMR 2024, that took place from 10-14 June 2024, in Phuket, Thailand.Effectively and efficiently retrieving information from multimedia collections (e.g., text, image, video, audio, sensor data, 3D) based on user needs is one of the most exciting areas in multimedia research. The Annual ACM International Conference on Multimedia Retrieval (ICMR) offers a great opportunity for exchanging leading-edge multimedia retrieval ideas among researchers, practitioners, and other potential users of multimedia retrieval systems. ACM ICMR was created in 2011 in a merger of ACM CIVR (International Conference on Image and Video Retrieval) and ACM MIR (International Conference on Multimedia Information Retrieval). ACM ICMR serves to illuminate the state of the art in multimedia retrieval. ACM ICMR 2024 in Phuket follows the successful previous editions of ICMR in Trento, Italy 2011; Hong Kong, China 2012; Dallas, USA 2013; Glasgow, UK 2014; Shanghai, China 2015; New York, USA 2016; Bucharest, Romania 2017; Yokohama, Japan 2018; Ottawa, Canada 2019; Dublin, Ireland 2020 (online); Taipei, Taiwan 2021 (online); Newark, USA 2022 (hybrid); and Thessaloniki, Greece 2023 (hybrid).},
location = {Phuket, Thailand}
}

@proceedings{10.1145/3630202,
title = {CoNEXT-SW '23: Proceedings of the on CoNEXT Student Workshop 2023},
year = {2023},
isbn = {9798400704529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 19th edition of the ACM Conference on Emerging Networking Experiment and Technologies (ACM CoNEXT 2022). CoNEXT is a premier and highly selective venue in computer networking. The first edition of the conference was organized in Toulouse in 2005 and we are back in France, but this time in Paris.Last year, the CoNEXT Steering Committee decided to move from a traditional conference model with a single submission deadline in June to a hybrid model with two submission deadlines (late November and late June). Two types of papers can be submitted to CoNEXT: (i) long papers presenting significant and novel research results on emerging computer networks and applications and (ii) short papers for contributions whose novelty and impact show the same technical excellence, but whose description fits within 6 pages. The accepted long papers are published in the journal Proceedings of the ACM on Networking (PACMNET) while the short papers appear in the conference proceedings.},
location = {Paris, France}
}

@proceedings{10.1145/3661638,
title = {AISNS '23: Proceedings of the 2023 International Conference on Artificial Intelligence, Systems and Network Security},
year = {2023},
isbn = {9798400716966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Mianyang, China}
}

@proceedings{10.1145/3630590,
title = {AINTEC '23: Proceedings of the 18th Asian Internet Engineering Conference},
year = {2023},
isbn = {9798400709395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hanoi, Vietnam}
}

@inproceedings{10.1145/3664476.3664507,
author = {Ruggiero, Claudia and Mazzini, Pietro and Coppa, Emilio and Lenti, Simone and Bonomi, Silvia},
title = {SoK: A Unified Data Model for Smart Contract Vulnerability Taxonomies},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3664507},
doi = {10.1145/3664476.3664507},
abstract = {Modern blockchains support the execution of application-level code in the form of smart contracts, allowing developers to devise complex Distributed Applications (DApps). Smart contracts are typically written in high-level languages, such as Solidity, and after deployment on the blockchain, their code is executed in a distributed way in response to transactions or calls from other smart contracts. As a common piece of software, smart contracts are susceptible to vulnerabilities, posing security threats to DApps and their users. The community has already made many different proposals involving taxonomies related to smart contract vulnerabilities. In this paper, we try to systematize such proposals, evaluating their common traits and main discrepancies. A major limitation emerging from our analysis is the lack of a proper formalization of such taxonomies, making hard their adoption within, e.g., tools and disfavoring their improvement over time as a community-driven effort. We thus introduce a novel data model that clearly defines the key entities and relationships relevant to smart contract vulnerabilities. We then show how our data model and its preliminary instantiation can effectively support several valuable use cases, such as interactive exploration of the taxonomy, integration with security frameworks for effective tool orchestration, and statistical analysis for performing longitudinal studies.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {43},
numpages = {13},
keywords = {Blockchain, Smart Contract, Taxonomy, Vulnerability, Weakness},
location = {Vienna, Austria},
series = {ARES '24}
}

@proceedings{10.1145/3629606,
title = {CHCHI '23: Proceedings of the Eleventh International Symposium of Chinese CHI},
year = {2023},
isbn = {9798400716454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denpasar, Bali, Indonesia}
}

@article{10.1613/jair.1.12397,
author = {Shahar, Tomer and Shekhar, Shashank and Atzmon, Dor and Saffidine, Abdallah and Juba, Brendan and Stern, Roni},
title = {Safe Multi-Agent Pathfinding with Time Uncertainty},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12397},
doi = {10.1613/jair.1.12397},
abstract = {In many real-world scenarios, the time it takes for a mobile agent, e.g., a robot, to move from one location to another may vary due to exogenous events and be difficult to predict accurately. Planning in such scenarios is challenging, especially in the context of Multi-Agent Pathfinding (MAPF), where the goal is to find paths to multiple agents and temporal coordination is necessary to avoid collisions. In this work, we consider a MAPF problem with this form of time uncertainty, where we are only given upper and lower bounds on the time it takes each agent to move. The objective is to find a safe solution, which is a solution that can be executed by all agents and is guaranteed to avoid collisions. We propose two complete and optimal algorithms for finding safe solutions based on well-known MAPF algorithms, namely, A* with Operator Decomposition (A* + OD) and Conflict-Based Search (CBS). Experimentally, we observe that on several standard MAPF grids the CBS-based algorithm performs better. We also explore the option of online replanning in this context, i.e., modifying the agents' plans during execution, to reduce the overall execution cost. We consider two online settings: (a) when an agent can sense the current time and its current location, and (b) when the agents can also communicate seamlessly during execution. For each setting, we propose a replanning algorithm and analyze its behavior theoretically and empirically. Our experimental evaluation confirms that indeed online replanning in both settings can significantly reduce solution cost.},
journal = {J. Artif. Int. Res.},
month = {may},
pages = {923–954},
numpages = {32}
}

@inproceedings{10.1145/3232174.3232175,
author = {Kunasri, Kasem and Panmanee, Chanita and Singkharat, Sombat and Tansuchat, Roengchai},
title = {Forecasting of Thailand and Myanmar Border Trade Value for Strategic Planning},
year = {2018},
isbn = {9781450364232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3232174.3232175},
doi = {10.1145/3232174.3232175},
abstract = {Forecasting the values of border trade are needed for strategic planning, especially in the competitive enhancement strategy. This paper applies the autoregressive integrated moving average (ARIMA) models to forecast the border trade value between Thailand and Myanmar on a monthly data basis. The data used are ranged from 2007 to 2016. The results bring about the forecasting model for the further border trade investment of both countries which is useful for making the decision on part of the entrepreneurs, investors, exporters, and importers. Furthermore, the relevant agencies can use these findings to determine the promoting directions of border trade in the future.},
booktitle = {Proceedings of the 2018 International Conference on Computers in Management and Business},
pages = {3–8},
numpages = {6},
keywords = {ARIMA, bordered trade value, forecasting, strategic planning},
location = {Oxford, United Kingdom},
series = {ICCMB '18}
}

@inproceedings{10.1145/3383219.3383276,
author = {Li, Shanshan and Xu, Qianwen and Hou, Peiyu and Chen, Xiudi and Wang, Yanze and Zhang, He and Rong, Guoping},
title = {Exploring the Challenges of Developing and Operating Consortium Blockchains: A Case Study},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383276},
doi = {10.1145/3383219.3383276},
abstract = {Blockchain and smart contracts are being embraced by more and more industrial practitioners in multiple domains including agriculture, manufacturing, and healthcare. As a distributed, immutable, and partly public ledger, the consortium blockchain demonstrates its potential to enable trustworthy interoperability and collaboration between organizations. However, the mismatch between the unruled software engineering practices and the increased interest of the consortium blockchain technology may pose threats to the quality of systems implemented. To mitigate the possible threats, this study takes the angle of software engineering to systematically understand the challenges and possible solutions in terms of developing and operating a consortium blockchain-based system. For this purpose, we conducted a case study on a typical consortium blockchain-based system and exhaustively collected the data by two rounds in-depth interviews on practitioners of different roles in the case project. Based on the data analysis, eight pairs of challenges and potential solutions were identified, which cover the phases of the development and operation of consortium blockchains. Moreover, we also captured two implications after further analysis of the findings, which worth the special attention of researchers in the near future, i.e. DevOps and microservices for blockchain or smart contracts.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {398–404},
numpages = {7},
keywords = {Consortium blockchain, DevOps, microservices, smart contracts},
location = {Trondheim, Norway},
series = {EASE '20}
}

@proceedings{10.1145/3665320,
title = {DigiPro '24: Proceedings of the 2024 Digital Production Symposium},
year = {2024},
isbn = {9798400706905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@article{10.1145/3623377,
author = {Hickling, Thomas and Zenati, Abdelhafid and Aouf, Nabil and Spencer, Phillippa},
title = {Explainability in Deep Reinforcement Learning: A Review into Current Methods and Applications},
year = {2023},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3623377},
doi = {10.1145/3623377},
abstract = {The use of Deep Reinforcement Learning (DRL) schemes has increased dramatically since their first introduction in 2015. Though uses in many different applications are being found, they still have a problem with the lack of interpretability. This has bread a lack of understanding and trust in the use of DRL solutions from researchers and the general public. To solve this problem, the field of Explainable Artificial Intelligence has emerged. This entails a variety of different methods that look to open the DRL black boxes, ranging from the use of interpretable symbolic Decision Trees to numerical methods like Shapley Values. This review looks at which methods are being used and for which applications. This is done to identify which models are the best suited to each application or if a method is being underutilised.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {125},
numpages = {35},
keywords = {Deep reinforcement learning, DRL, Explainable AI, XAI, neural networks, Survey, Review}
}

@inproceedings{10.1145/3600160.3604997,
author = {Sayeed, Sarwar and Pitropakis, Nikolaos and Buchanan, William J. and Markakis, Evangelos and Papatsaroucha, Dimitra and Politis, Ilias},
title = {TRUSTEE: Towards the creation of secure, trustworthy and privacy-preserving framework},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600160.3604997},
doi = {10.1145/3600160.3604997},
abstract = {Digital transformation is a method where new technologies replace the old to meet essential organisational requirements and enhance the end-user experience. Technological transformation often improvises the manner in which a facility or resources are delivered to the recipient. Data is one of the key assets of every organisation which influences significantly reaching the long-term objective. Thus, the entities, as well as technologies involved in the data management process, have a significant role to play to secure different data types. However, the traditional data governance process often follows a centralised approach and thus resulting in various cyber attacks, whereas the distributed approaches are mostly research prototypes and often comprise various security challenges. Security incidents such as data theft fabricate the integrity of confidential data and thus the consequences are often disastrous. To address the challenges, we introduce TRUSTEE, a data-driven platform which aims to provide a secure and privacy-by-design framework to empower companies, organisations, and individuals to access different data domains, use and re-use the data and metadata to extract knowledge with trust and confidentiality. In this paper, we assess the effectiveness of the platform by reviewing the potential challenges and threats associated with the incorporated technologies. Our research emphasises the efficacy of distributed technologies to indicate their significance in data integrity and security.},
booktitle = {Proceedings of the 18th International Conference on Availability, Reliability and Security},
articleno = {145},
numpages = {10},
keywords = {Distributed Ledger, IPFS, Privacy, Security, Trust},
location = {Benevento, Italy},
series = {ARES '23}
}

@inproceedings{10.1145/2631890.2631895,
author = {Portillo-Dominguez, A. Omar and Wang, Miao and Murphy, John and Magoni, Damien and Mitchell, Nick and Sweeney, Peter F. and Altman, Erik},
title = {Towards an automated approach to use expert systems in the performance testing of distributed systems},
year = {2014},
isbn = {9781450329330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2631890.2631895},
doi = {10.1145/2631890.2631895},
abstract = {Performance testing in distributed environments is challenging. Specifically, the identification of performance issues and their root causes are time-consuming and complex tasks which heavily rely on expertise. To simplify these tasks, many researchers have been developing tools with built-in expertise. However limitations exist in these tools, such as managing huge volumes of distributed data, that prevent their efficient usage for performance testing of highly distributed environments. To address these limitations, this paper presents an adaptive framework to automate the usage of expert systems in performance testing. Our validation assessed the accuracy of the framework and the time savings that it brings to testers. The results proved the benefits of the framework by achieving a significant decrease in the time invested in performance analysis and testing.},
booktitle = {Proceedings of the 2014 Workshop on Joining AcadeMiA and Industry Contributions to Test Automation and Model-Based Testing},
pages = {22–27},
numpages = {6},
keywords = {Automation, Distributed Systems, Expert Systems, Performance Analysis, Performance Testing},
location = {San Jose, CA, USA},
series = {JAMAICA 2014}
}

@inproceedings{10.1145/3439961.3439964,
author = {Moreira, Jaziel S. and Alves, Everton L. G. and Andrade, Wilkerson L.},
title = {A Systematic Mapping on Energy Efficiency Testing in Android Applications},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439964},
doi = {10.1145/3439961.3439964},
abstract = {Android devices include a wide range of features and functionalities. However, they are limited by their battery capacity. Energy efficiency has become a critical non-functional requirement for Android applications. Most applications use multiple hardware elements that may consume a great amount of energy. Moreover, energy faults and bad resource management may aggravate this issue. Several works have proposed solutions to help developers deal with energy consumption issues. In this work, we present a systematic mapping study on energy efficiency testing for Android applications. From a starting set of 1525 papers, we narrowed our investigation to 32 relevant ones. The most common research topics were Fine-grained Estimation with nine studies, followed by Test Generation and Classification, both with six studies. We also found that most apply only dynamic solutions and use software-based strategies to estimate energy consumption. Finally, we discuss a series of open problems that should be addressed by future research.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {3},
numpages = {10},
keywords = {Android Application, Energy Efficiency, Testing},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@article{10.1145/3665794,
author = {Qiu, Bing and Huo, Jiahao},
title = {Quantitative Stylistic Analysis of Middle Chinese Texts Based on the Dissimilarity of Evolutive Core Word Usage},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3665794},
doi = {10.1145/3665794},
abstract = {Stylistic analysis enables open-ended and exploratory observation of languages. To fill the gap in the quantitative analysis of the stylistic systems of Middle Chinese, we construct lexical features based on the evolutive core word usage and scheme a Bayesian method for feature parameters estimation. The lexical features are from the Swadesh list, each of which has different word forms along with the language evolution during the Middle Ages. We thus count the varied word of those entries along with the language evolution as the linguistic features. With the Bayesian formulation, the feature parameters are estimated to construct a high-dimensional random feature vector to obtain the pair-wise dissimilarity matrix of all the texts based on different distance measures. Finally, we perform the spectral embedding and clustering to visualize, categorize, and analyze the linguistic styles of Middle Chinese texts. The quantitative result agrees with the existing qualitative conclusions and, furthermore, betters our understanding of the linguistic styles of Middle Chinese from both the inter-category and intra-category aspects. It also helps unveil the special styles induced by the indirect language contact.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
articleno = {97},
numpages = {22},
keywords = {Stylistic analysis, middle chinese, swadesh list, lexical feature}
}

@article{10.1145/3652159,
author = {Akanova, Akerke and Ismailova, Aisulu and Oralbekova, Zhanar and Kenzhebayeva, Zhanat and Anarbekova, Galiya},
title = {Neurocomputer System of Semantic Analysis of the Text in the Kazakh Language},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3652159},
doi = {10.1145/3652159},
abstract = {The purpose of the study is to solve an extreme mathematical problem—semantic analysis of natural language, which can be used in various fields, including marketing research, online translators, and search engines. When training the neural network, data training methods based on the latent Dirichlet allocation model and vector representation of words were used. This study presents the development of a neurocomputer system used for the purpose of semantic analysis of the text in the Kazakh language, based on machine learning and the use of the latent Dirichlet allocation model. In the course of the study, the stages of system development were considered, regarding the text recognition algorithm. The Python programming language was used as a tool using libraries that greatly simplify the process of creating neural networks, including the Keras library. An experiment was conducted with the involvement of experts to test the effectiveness of the system, the results of which confirmed the reliability of the data provided by the system. The papers of modern computer linguists dealing with the problems of natural language processing using various technologies and methods are considered.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {57},
numpages = {15},
keywords = {Machine learning, latent Dirichlet allocation, Python, Keras library, automatic recognition}
}

@inproceedings{10.1145/3358695.3360941,
author = {Liu, Liangliang and Fu, Xue and Liu, Haibo and Cao, Xinyu and Wang, Haitao},
title = {Automatic Construction of Chinese Typo-Pairs Based on Web Corpus},
year = {2019},
isbn = {9781450369886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358695.3360941},
doi = {10.1145/3358695.3360941},
abstract = {With the development of big data, the amount of text data is growing bigger and bigger in which errors are also more and more. The traditional human-correction cannot meet the actual demand. It is a trend for automatic text proofing by using computer data processing. Chinese text errors can be divided into two categories: non-word error and real-word error. One or more character in a Chinese word replaced by other character will result in the word does not belong to the Chinese dictionary, which we call "non-word error". The word segmentation is firstly performed on the corpus in Chinese NLP, and non-word error will be divided into several disperse strings, which bring Chinese text proofreading several problems, because there are single-character words and multi-characters words in Chinese dictionary. In this paper, an approach is proposed to construct Chinese typo-pairs from Web corpus, which can be used in Chinese text automatic proofreading efficiently. Firstly, the method adds similar words into a candidate set using fuzzy matching algorithm, and then validates the similar words in the candidate set using statistical models, and finally constructs the typo-pairs.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume},
pages = {159–164},
numpages = {6},
keywords = {Non-wordError, Pattern Matching, Real-wordError, TextProofreading, Typo-pairs},
location = {Thessaloniki, Greece},
series = {WI '19 Companion}
}

@inproceedings{10.1145/3494106.3528673,
author = {Mullen, Tony and Finn, P. D.},
title = {Towards an Evaluation Metric for Carbon-Emitting Energy Provenance of Bitcoin Transactions},
year = {2022},
isbn = {9781450391757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494106.3528673},
doi = {10.1145/3494106.3528673},
abstract = {We present an approach to evaluating the carbon-emitting energy provenance of Bitcoin transactions and transaction outputs. Our approach incorporates published global energy production data and existing state-of-the-art estimates of Bitcoin energy consumption into a scoring algorithm for individual mined blocks. We then present two proposals for deriving scores for transactions based on the coinbase origins of the Bitcoin currency values of the transactions' inputs. The first proposal is comparatively simple, and weights coinbase origin contributions to a transaction based on recency in transaction hops from its origin block. The second proposal adjusts the weights of coinbase contributions at each intermediary transaction based on the input and output values of those transactions. Using these methods we are able to associate individual transactions and unspent transaction outputs with specific quantities of atmospheric carbon. Finally, we offer an outline of an incentivization strategy in the form of a blockchain-based carbon-offsetting oracle that would track the creation and exchange of offsets based on the metrics proposed.},
booktitle = {Proceedings of the Fourth ACM International Symposium on Blockchain and Secure Critical Infrastructure},
pages = {11–21},
numpages = {11},
keywords = {bitcoin, blockchain, carbon emission, carbon offset, coinbase, energy provenance, mining},
location = {Nagasaki, Japan},
series = {BSCI '22}
}

@proceedings{10.1145/3636534,
title = {ACM MobiCom '24: Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
year = {2024},
isbn = {9798400704895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Washington D.C., DC, USA}
}

@proceedings{10.1145/3641399,
title = {ISEC '24: Proceedings of the 17th Innovations in Software Engineering Conference},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@proceedings{10.1145/3600160,
title = {ARES '23: Proceedings of the 18th International Conference on Availability, Reliability and Security},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Benevento, Italy}
}

@inproceedings{10.1145/3597926.3598057,
author = {Ye, Mingxi and Nan, Yuhong and Zheng, Zibin and Wu, Dongpeng and Li, Huizhong},
title = {Detecting State Inconsistency Bugs in DApps via On-Chain Transaction Replay and Fuzzing},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598057},
doi = {10.1145/3597926.3598057},
abstract = {Decentralized applications (DApps) consist of multiple smart contracts running on Blockchain. With the increasing popularity of the DApp ecosystem, vulnerabilities in DApps could bring significant impacts such as financial losses. Identifying vulnerabilities in DApps is by no means trivial, as modern DApps consist of complex interactions across multiple contracts. Previous research suffers from either high false positives or false negatives, due to the lack of precise contextual information which is mandatory for confirming smart contract vulnerabilities when analyzing smart contracts. In this paper, we present IcyChecker, a new fuzzing-based framework to effectively identify State inconsistency (SI) Bugs – a specific type of bugs that can cause vulnerabilities such as re-entrancy, front-running with complex patterns. Different from prior works, IcyChecker utilizes a set of accurate contextual information for contract fuzzing by replaying the on-chain historical transactions. Besides, instead of designing specific testing oracles which are required by other fuzzing approaches, IcyChecker implements novel mechanisms to mutate a set of fuzzing transaction sequences, and further identify SI bugs by observing their state differences. Evaluation of IcyChecker over the top 100 popular DApps showed it effectively identifies a total number of 277 SI bugs, with a precision of 87\%. By comparing IcyChecker with other state-of-the-art tools (i.e., Smartian, Confuzzius, and Sailfish), we show IcyChecker not only identifies more SI bugs but also with much lower false positives, thanks to its integration of accurate on-chain data and unique fuzzing strategies. Our research sheds light on new ways of detecting smart contract vulnerabilities in DApps.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {298–309},
numpages = {12},
keywords = {Decentralized Application, Fuzz Testing, Smart Contract, Vulnerability detection},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3460319.3464837,
author = {Ren, Meng and Yin, Zijing and Ma, Fuchen and Xu, Zhenyang and Jiang, Yu and Sun, Chengnian and Li, Huizhong and Cai, Yan},
title = {Empirical evaluation of smart contract testing: what is the best choice?},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464837},
doi = {10.1145/3460319.3464837},
abstract = {Security of smart contracts has attracted increasing attention in recent years. Many researchers have devoted themselves to devising testing tools for vulnerability detection. Each published tool has demonstrated its effectiveness through a series of evaluations on their own experimental scenarios. However, the inconsistency of evaluation settings such as different data sets or performance metrics, may result in biased conclusion.  In this paper, based on an empirical evaluation of widely used smart contract testing tools, we propose a unified standard to eliminate the bias in the assessment process. First, we collect 46,186 source-available smart contracts from four influential organizations. This comprehensive dataset is open to the public and involves different code characteristics, vulnerability patterns and application scenarios. Then we propose a 4-step evaluation process and summarize the difference among relevant work in these steps. We use nine representative tools to carry out extensive experiments. The results demonstrate that different choices of experimental settings could significantly affect tool performance and lead to misleading or even opposite conclusions. Finally, we generalize some problems of existing testing tools, and propose some possible directions for further improvement.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {566–579},
numpages = {14},
keywords = {evaluation, observations and solutions, smart contract testing},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}


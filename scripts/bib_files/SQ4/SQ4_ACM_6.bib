@article{10.1162/EVCO_a_00141,
author = {Silva, Fernando and Urbano, Paulo and Correia, Lu\'{\i}s and Christensen, Anders Lyhne},
title = {Odneat: An algorithm for decentralised online evolution of robotic controllers},
year = {2015},
issue_date = {Fall 2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {23},
number = {3},
issn = {1063-6560},
url = {https://doi.org/10.1162/EVCO_a_00141},
doi = {10.1162/EVCO_a_00141},
abstract = {Online evolution gives robots the capacity to learn new tasks and to adapt to changing environmental conditions during task execution. Previous approaches to online evolution of neural controllers are typically limited to the optimisation of weights in networks with a prespecified, fixed topology. In this article, we propose a novel approach to online learning in groups of autonomous robots called odNEAT. odNEAT is a distributed and decentralised neuroevolution algorithm that evolves both weights and network topology. We demonstrate odNEAT in three multirobot tasks: aggregation, integrated navigation and obstacle avoidance, and phototaxis. Results show that odNEAT approximates the performance of rtNEAT, an efficient centralised method, and outperforms IM-&lt;inline-formula&gt;&lt;inline-graphic xlink="EVCO_a_00141inline1.gif" xlink:type="simple"/&gt;&lt;/inline-formula&gt;, a decentralised neuroevolution algorithm. Compared with rtNEAT and IM-&lt;inline-formula&gt;&lt;inline-graphic xlink="EVCO_a_00141inline2.gif" xlink:type="simple"/&gt;&lt;/inline-formula&gt;, odNEAT's evolutionary dynamics lead to the synthesis of less complex neural controllers with superior generalisation capabilities. We show that robots executing odNEAT can display a high degree of fault tolerance as they are able to adapt and learn new behaviours in the presence of faults. We conclude with a series of ablation studies to analyse the impact of each algorithmic component on performance.},
journal = {Evol. Comput.},
month = {sep},
pages = {421–449},
numpages = {29},
keywords = {online evolution, neurocontroller, multirobot systems, decentralised algorithms, Artificial neural networks}
}

@inproceedings{10.1145/3423211.3425693,
author = {Yang, Donglin and Rang, Wei and Cheng, Dazhao},
title = {Mitigating Stragglers in the Decentralized Training on Heterogeneous Clusters},
year = {2020},
isbn = {9781450381536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423211.3425693},
doi = {10.1145/3423211.3425693},
abstract = {Decentralized algorithms, e.g., AllReduce, have been widely applied as the synchronization strategy for data-parallel distributed deep learning due to its superior performance over centralized ones. The synchronous Stochastic Gradient Descent (SGD) approach guarantees accuracy for various deep learning models, but its performance suffers from stragglers, i.e., "long-tail effects." The straggler can be caused by the inherent load imbalance from workloads or system heterogeneity. Despite existing optimizations to support centralized algorithms against stragglers, little effort has been explored in decentralized training algorithms.This paper proposes a Randomized Non-blocking AllReduce (RNA) protocol to mitigate the straggler problem. To avoid "long-tail effects" brought by the strict barrier in the AllReduce, we propose a decentralized, relaxed, and randomized sampling approach to implement partial AllReduce operation. To handle heterogeneity at a large scale, we combine the traditional Parameter Servers (PS) with AllReduce to implement a hierarchical synchronization mechanism. We theoretically demonstrate the convergence analysis and detail the system implementation. The experiment results on representative deep learning models show nearly 1.8\texttimes{} speedup over the state-of-the-art Horovod and 1.3\texttimes{} speedup over AD-PSGD on a heterogeneous cluster.},
booktitle = {Proceedings of the 21st International Middleware Conference},
pages = {386–399},
numpages = {14},
keywords = {Heterogeneity, Distributed deep learning, Decentralized training},
location = {Delft, Netherlands},
series = {Middleware '20}
}

@inproceedings{10.1145/1967486.1967505,
author = {Yu, Weihai},
title = {Fault handling and recovery in decentralized services orchestration},
year = {2010},
isbn = {9781450304214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1967486.1967505},
doi = {10.1145/1967486.1967505},
abstract = {Today, orchestration of composite services is typically carried out by dedicated central engines. A central engine, however, can easily become a performance bottleneck. Furthermore, centralized orchestration is infeasible for the composite of services beyond enterprise boundaries. Decentralized orchestration approaches are therefore devised to overcome these drawbacks. Decentralized orchestration, however, is generally regarded as more complex for certain orchestration management tasks due to absence of centralized states. Among the challenging tasks are fault handling and recovery. This paper presents fault handling and recovery of composite services in a decentralized orchestration approach that is based on continuation-passing messaging. The experimental results show performance advantage of the approach even in presence of service faults.},
booktitle = {Proceedings of the 12th International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {98–105},
numpages = {8},
keywords = {compensation, continuation-passing messaging, decentralized orchestration, fault handling, recovery},
location = {Paris, France},
series = {iiWAS '10}
}

@inproceedings{10.1145/3377049.3377136,
author = {Khan, Samiur and Al-Amin, Md. and Hossain, Hrittik and Noor, Nawshad and Sadik, Md. Wahid},
title = {A Pragmatical Study on Blockchain Empowered Decentralized Application Development Platform},
year = {2020},
isbn = {9781450377782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377049.3377136},
doi = {10.1145/3377049.3377136},
abstract = {Blockchain has been a latest trend and many of its applications show promising results in terms of transactions, validations, finances etc. and recently it has become an apple of eye for the tech industry and investors. The concept of decentralized application (commonly known as dApp) and the blockchain platform itself can help to reform many unresolved solutions for the betterment of services and generate new opportunities for the developers to focus on creating applications in a decentralized environment with ease. The adaptation of blockchain technology through dApps is increasing drastically; thus opportunity for the developers and entrepreneurs are growing rapidly. Considering the impactful revolution of dApps, the developer communities are getting more interest to the dApps development tools, frameworks and platforms. This paper is presenting a comprehensive study on some popular blockchain powered decentralized or distributed dApp platforms. In this paper we made analysis of these platforms based on different factors and parameters. This analysis provides an impactful direction to the developer and enterprise community in adopting of the development platform.},
booktitle = {Proceedings of the International Conference on Computing Advancements},
articleno = {82},
numpages = {9},
keywords = {Blockchain, Corda, Decentralized Application, Development Platform, Ethereum, Hyperledger Fabric, dApps},
location = {Dhaka, Bangladesh},
series = {ICCA 2020}
}

@inproceedings{10.1145/3373376.3378499,
author = {Luo, Qinyi and He, Jiaao and Zhuo, Youwei and Qian, Xuehai},
title = {Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378499},
doi = {10.1145/3373376.3378499},
abstract = {Distributed deep learning training usually adopts All-Reduce as the synchronization mechanism for data parallel algorithms due to its high performance in homogeneous environment. However, its performance is bounded by the slowest worker among all workers. For this reason, it is significantly slower in heterogeneous settings. AD-PSGD, a newly proposed synchronization method which provides numerically fast convergence and heterogeneity tolerance, suffers from deadlock issues and high synchronization overhead. Is it possible to get the best of both worlds --- designing a distributed training method that has both high performance like All-Reduce in homogeneous environment and good heterogeneity tolerance like AD-PSGD?In this paper, we propose Prague, a high-performance heterogeneity-aware asynchronous decentralized training approach. We achieve the above goal with intensive synchronization optimization by exploring the interplay between algorithm and system implementation, or statistical and hardware efficiency. To reduce synchronization cost, we propose a novel communication primitive, Partial All-Reduce, that enables fast synchronization among a group of workers. To reduce serialization cost, we propose static group scheduling in homogeneous environment and simple techniques, i.e., Group Buffer and Group Division, to largely eliminate conflicts with slightly reduced randomness. Our experiments show that in homogeneous environment, Prague is 1.2x faster than the state-of-the-art implementation of All-Reduce, 5.3x faster than Parameter Server and 3.7x faster than AD-PSGD. In a heterogeneous setting, Prague tolerates slowdowns well and achieves 4.4x speedup over All-Reduce.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {401–416},
numpages = {16},
keywords = {machine learning, heterogeneity, deep learning, decentralized training},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@article{10.1145/1273445.1273460,
author = {Gavras, Anastasius and Karila, Arto and Fdida, Serge and May, Martin and Potts, Martin},
title = {Future internet research and experimentation: the FIRE initiative},
year = {2007},
issue_date = {July 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/1273445.1273460},
doi = {10.1145/1273445.1273460},
abstract = {The research community worldwide has increasingly drawn its attention to the weaknesses of the current Internet. Many proposals are addressing the perceived problems, ranging from new enhanced protocols to fix specific problems up to the most radical proposal to redesign and deploy a fully new Internet. Most of the problems in the current Internet are rooted in the tremendous pace of increase of its use. As a consequence there was little time to address the deficiencies of the Internet from an architectural point of view.Within FP7, the European Commission has facilitated the creation of European expert groups around the theme FIRE "Future Internet Research and Experimentation". FIRE has two related dimensions: on one hand, promoting experimentally-driven long-term, visionary research on new paradigms and networking concepts and architectures for the future Internet; on the other hand, building a large-scale experimentation facility supporting both medium- and long-term research on networks and services by gradually federating existing and new testbeds for emerging or future Internet technologies. By addressing future challenges for the Internet such as mobility, scalability, security and privacy, this new experimentally-driven approach is challenging the mainstream perceptions for future Internet development. This new initiative is intended to complement the more industrially-driven approaches which are addressed under the FP7 Objective "The Network of the Future" within the FP7-ICT Workprogramme 2007-08. FIRE is focused on exploring new and radically better technological solutions for the future Internet, while preserving the "good" aspects of the current Internet, in terms of openness, freedom of expression and ubiquitous access. The FIRE activities are being launched in the 2nd ICT call, which closes in October 2007, under the FP7-ICT Objective 1.6 "New Paradigms and Experimental Facilities" (budget ε40m). Projects are envisaged to start in early 2008.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {jul},
pages = {89–92},
numpages = {4},
keywords = {testbed, situated and autonomic communications, network neutrality, network architecture, future internet, experimentation}
}

@proceedings{10.1145/3647722,
title = {ICSIM '24: Proceedings of the 2024 7th International Conference on Software Engineering and Information Management},
year = {2024},
isbn = {9798400709197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Suva, Fiji}
}

@inproceedings{10.1109/SEAMS.2017.8,
author = {Pilgerstorfer, Peter and Pournaras, Evangelos},
title = {Self-adaptive learning in decentralized combinatorial optimization: a design paradigm for sharing economies},
year = {2017},
isbn = {9781538615508},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2017.8},
doi = {10.1109/SEAMS.2017.8},
abstract = {The democratization of Internet of Things and ubiquitous computing equips citizens with phenomenal new ways for online participation and decision-making in application domains of smart grids and smart cities. When agents autonomously self-determine the options from which they make choices, while these choices collectively have an overall system-wide impact, an optimal decision-making turns into a combinatorial optimization problem known to be NP-hard. This paper contributes a new generic self-adaptive learning algorithm for a fully decentralized combinatorial optimization: I-EPOS, the Iterative Economic Planning and Optimized Selections. In contrast to related algorithms that simply parallelize computations or big data and deep learning systems that often require personal data and overtake of control with implication on privacy-preservation and autonomy, I-EPOS relies on coordinated local decision-making via structured interactions over tree topologies that involve the exchange of entirely local and aggregated information. Strikingly, the cost-effectiveness of I-EPOS in regards to performance vs. computational and communication cost highly outperforms other related algorithms that involve non-local brute-force operations or exchange of full information. The algorithm is also evaluated using real-world data from two state-of-the-art pilot projects of participatory sharing economies: (i) energy management and (ii) bicycle sharing. The contribution of an I-EPOS open source software suite implemented as a paradigmatic artifact for community aspires to settle a knowledge exchange for the design of new algorithms and application scenarios of sharing economies towards highly participatory and sustainable digital societies.},
booktitle = {Proceedings of the 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {54–64},
numpages = {11},
keywords = {smart grid, smart city, sharing economy, optimization, network, learning, decentralized system, adaptation},
location = {Buenos Aires, Argentina},
series = {SEAMS '17}
}

@inproceedings{10.1145/55482.55508,
author = {Stumm, M.},
title = {Strategies for decentralized resource management},
year = {1987},
isbn = {0897912454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/55482.55508},
doi = {10.1145/55482.55508},
abstract = {Decentralized resource management in distributed systems has become more practical with the availability of communication facilities that support multicasting. In this paper we present several example solutions for managing resources in a decentralized fashion, using multicasting facilities. We review the properties of these solutions in terms of scalability, fault tolerance and efficiency. We conclude that decentralized solutions compare favorably to centralized solutions with respect to all three criteria.},
booktitle = {Proceedings of the ACM Workshop on Frontiers in Computer Communications Technology},
pages = {245–253},
numpages = {9},
location = {Stowe, Vermont, USA},
series = {SIGCOMM '87}
}

@article{10.1145/55483.55508,
author = {Stumm, M.},
title = {Strategies for decentralized resource management},
year = {1987},
issue_date = {Oct./Nov. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {5},
issn = {0146-4833},
url = {https://doi.org/10.1145/55483.55508},
doi = {10.1145/55483.55508},
abstract = {Decentralized resource management in distributed systems has become more practical with the availability of communication facilities that support multicasting. In this paper we present several example solutions for managing resources in a decentralized fashion, using multicasting facilities. We review the properties of these solutions in terms of scalability, fault tolerance and efficiency. We conclude that decentralized solutions compare favorably to centralized solutions with respect to all three criteria.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {245–253},
numpages = {9}
}

@inproceedings{10.1145/291069.291061,
author = {Ranganathan, Narayan and Franklin, Manoj},
title = {An empirical study of decentralized ILP execution models},
year = {1998},
isbn = {1581131070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/291069.291061},
doi = {10.1145/291069.291061},
abstract = {Recent fascination for dynamic scheduling as a means for exploiting instruction-level parallelism has introduced significant interest in the scalability aspects of dynamic scheduling hardware. In order to overcome the scalability problems of centralized hardware schedulers, many decentralized execution models are being proposed and investigated recently. The crux of all these models is to split the instruction window across multiple processing elements (PEs) that do independent, scheduling of instructions. The decentralized execution models proposed so far can be grouped under 3 categories, based on the criterion used for assigning an instruction to a particular PE. They are: (i) execution unit dependence based decentralization (EDD), (ii) control dependence based decentralization (CDD), and (iii) data dependence based decentralization (DDD). This paper investigates the performance aspects of these three decentralization approaches. Using a suite of important benchmarks and realistic system parameters, we examine performance differences resulting from the type of partitioning as well as from specific implementation issues such as the type of PE interconnect.We found that with a ring-type PE interconnect, the DDD approach performs the best when the number of PEs is moderate, and that the CDD approach performs best when the number of PEs is large. The currently used approach---EDD---does not perform well for any configuration. With a realistic crossbar, performance does not increase with the number of PEs for any of the partitioning approaches. The results give insight into the best way to use the transistor budget available for implementing the instruction window.},
booktitle = {Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {272–281},
numpages = {10},
keywords = {speculative execution, instruction-level parallelism, hardware window, execution unit dependence, dynamic scheduling, decentralization, data dependence, control dependence},
location = {San Jose, California, USA},
series = {ASPLOS VIII}
}

@article{10.1145/291006.291061,
author = {Ranganathan, Narayan and Franklin, Manoj},
title = {An empirical study of decentralized ILP execution models},
year = {1998},
issue_date = {Nov. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {11},
issn = {0362-1340},
url = {https://doi.org/10.1145/291006.291061},
doi = {10.1145/291006.291061},
abstract = {Recent fascination for dynamic scheduling as a means for exploiting instruction-level parallelism has introduced significant interest in the scalability aspects of dynamic scheduling hardware. In order to overcome the scalability problems of centralized hardware schedulers, many decentralized execution models are being proposed and investigated recently. The crux of all these models is to split the instruction window across multiple processing elements (PEs) that do independent, scheduling of instructions. The decentralized execution models proposed so far can be grouped under 3 categories, based on the criterion used for assigning an instruction to a particular PE. They are: (i) execution unit dependence based decentralization (EDD), (ii) control dependence based decentralization (CDD), and (iii) data dependence based decentralization (DDD). This paper investigates the performance aspects of these three decentralization approaches. Using a suite of important benchmarks and realistic system parameters, we examine performance differences resulting from the type of partitioning as well as from specific implementation issues such as the type of PE interconnect.We found that with a ring-type PE interconnect, the DDD approach performs the best when the number of PEs is moderate, and that the CDD approach performs best when the number of PEs is large. The currently used approach---EDD---does not perform well for any configuration. With a realistic crossbar, performance does not increase with the number of PEs for any of the partitioning approaches. The results give insight into the best way to use the transistor budget available for implementing the instruction window.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {272–281},
numpages = {10},
keywords = {speculative execution, instruction-level parallelism, hardware window, execution unit dependence, dynamic scheduling, decentralization, data dependence, control dependence}
}

@article{10.1145/384265.291061,
author = {Ranganathan, Narayan and Franklin, Manoj},
title = {An empirical study of decentralized ILP execution models},
year = {1998},
issue_date = {Dec. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/384265.291061},
doi = {10.1145/384265.291061},
abstract = {Recent fascination for dynamic scheduling as a means for exploiting instruction-level parallelism has introduced significant interest in the scalability aspects of dynamic scheduling hardware. In order to overcome the scalability problems of centralized hardware schedulers, many decentralized execution models are being proposed and investigated recently. The crux of all these models is to split the instruction window across multiple processing elements (PEs) that do independent, scheduling of instructions. The decentralized execution models proposed so far can be grouped under 3 categories, based on the criterion used for assigning an instruction to a particular PE. They are: (i) execution unit dependence based decentralization (EDD), (ii) control dependence based decentralization (CDD), and (iii) data dependence based decentralization (DDD). This paper investigates the performance aspects of these three decentralization approaches. Using a suite of important benchmarks and realistic system parameters, we examine performance differences resulting from the type of partitioning as well as from specific implementation issues such as the type of PE interconnect.We found that with a ring-type PE interconnect, the DDD approach performs the best when the number of PEs is moderate, and that the CDD approach performs best when the number of PEs is large. The currently used approach---EDD---does not perform well for any configuration. With a realistic crossbar, performance does not increase with the number of PEs for any of the partitioning approaches. The results give insight into the best way to use the transistor budget available for implementing the instruction window.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {oct},
pages = {272–281},
numpages = {10},
keywords = {speculative execution, instruction-level parallelism, hardware window, execution unit dependence, dynamic scheduling, decentralization, data dependence, control dependence}
}

@article{10.1145/1525856.1525857,
author = {Kho, Johnsen and Rogers, Alex and Jennings, Nicholas R.},
title = {Decentralized control of adaptive sampling in wireless sensor networks},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/1525856.1525857},
doi = {10.1145/1525856.1525857},
abstract = {The efficient allocation of the limited energy resources of a wireless sensor network in a way that maximizes the information value of the data collected is a significant research challenge. Within this context, this article concentrates on adaptive sampling as a means of focusing a sensor's energy consumption on obtaining the most important data. Specifically, we develop a principled information metric based upon Fisher information and Gaussian process regression that allows the information content of a sensor's observations to be expressed. We then use this metric to derive three novel decentralized control algorithms for information-based adaptive sampling which represent a trade-off in computational cost and optimality. These algorithms are evaluated in the context of a deployed sensor network in the domain of flood monitoring. The most computationally efficient of the three is shown to increase the value of information gathered by approximately 83\%, 27\%, and 8\% per day compared to benchmarks that sample in a na\"{\i}ve nonadaptive manner, in a uniform nonadaptive manner, and using a state-of-the-art adaptive sampling heuristic (USAC) correspondingly. Moreover, our algorithm collects information whose total value is approximately 75\% of the optimal solution (which requires an exponential, and thus impractical, amount of time to compute).},
journal = {ACM Trans. Sen. Netw.},
month = {jun},
articleno = {19},
numpages = {35},
keywords = {information metric, decentralized decision mechanism, Gaussian process regression, Adaptive sampling algorithm}
}

@inproceedings{10.1145/3592149.3592151,
author = {Detzner, Peter and G\"{o}deke, Jana and T\"{o}nning, Lars and Laskowski, Patrick and H\"{o}rstrup, Maximilian and Stolz, Oliver and Brehler, Marius and Kerner, S\"{o}ren},
title = {SOLA: A Decentralized Communication Middleware Developed with ns-3},
year = {2023},
isbn = {9798400707476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592149.3592151},
doi = {10.1145/3592149.3592151},
abstract = {The transformation from static production facilities into a flexible and decentralized cyber-physical production system (CPPS) is part of the current ongoing Industry 4.0. A CPPS will enable and support communication between people, machines and virtual objects,&nbsp;e.g.,&nbsp;as material flow or products, alike. However, communication in CPPS relies often on centralized approaches using a message broker or is not considered at all. We present in this paper the decentralized communication middleware called SOLA &nbsp; with an emphasis on, but not limited to, CPPS. SOLA &nbsp;uses the inherent given capability to communicate of participating nodes and eliminates the need for a central instance. A structured overlay network is created and managed, which appears to its users as a single coherent system. The main building blocks of SOLA &nbsp;are the management overlay and the event dissemination. Within this building blocks, no single peer has a global view and all operations are based on each peer’s local view. The local view represents some selected links to a subset of all peers in the network. In addition to this, we also present how SOLA &nbsp;was developed with the help of the discrete-event simulator&nbsp;ns-3. Finally, we also show how we have used ns-3 to simulate a self-organizing material flow where participants use SOLA &nbsp;to communicate.},
booktitle = {Proceedings of the 2023 Workshop on Ns-3},
pages = {78–85},
numpages = {8},
keywords = {balanced tree, cyber-physical production system, decentral organized communication, peer-to-peer network},
location = {Arlington, VA, USA},
series = {WNS3 '23}
}

@article{10.1145/3447624,
author = {Rodionova, Al\"{e}na and Pant, Yash Vardhan and Kurtz, Connor and Jang, Kuk and Abbas, Houssam and Mangharam, Rahul},
title = {Learning-‘N-Flying: A Learning-Based, Decentralized Mission-Aware UAS Collision Avoidance Scheme},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3447624},
doi = {10.1145/3447624},
abstract = {Urban Air Mobility, the scenario where hundreds of manned and Unmanned Aircraft Systems (UASs) carry out a wide variety of missions (e.g., moving humans and goods within the city), is gaining acceptance as a transportation solution of the future. One of the key requirements for this to happen is safely managing the air traffic in these urban airspaces. Due to the expected density of the airspace, this requires fast autonomous solutions that can be deployed online. We propose Learning-‘N-Flying (LNF), a multi-UAS Collision Avoidance (CA) framework. It is decentralized, works on the fly, and allows autonomous Unmanned Aircraft System (UAS)s managed by different operators to safely carry out complex missions, represented using Signal Temporal Logic, in a shared airspace. We initially formulate the problem of predictive collision avoidance for two UASs as a mixed-integer linear program, and show that it is intractable to solve online. Instead, we first develop Learning-to-Fly (L2F) by combining (1) learning-based decision-making and (2) decentralized convex optimization-based control. LNF extends L2F to cases where there are more than two UASs on a collision path. Through extensive simulations, we show that our method can run online (computation time in the order of milliseconds) and under certain assumptions has failure rates of less than 1\% in the worst case, improving to near 0\% in more relaxed operations. We show the applicability of our scheme to a wide variety of settings through multiple case studies.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {sep},
articleno = {35},
numpages = {26},
keywords = {Model Predictive Control, neural network, robustness, temporal logic, unmanned aircraft systems, Collision avoidance}
}

@inproceedings{10.1145/1281700.1281720,
author = {Lo, Samantha and Chang, Rocky K. C. and Colitti, Lorenzo},
title = {An active approach to measuring routing dynamics induced by autonomous systems},
year = {2007},
isbn = {9781595937513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281700.1281720},
doi = {10.1145/1281700.1281720},
abstract = {We present an active measurement study of the routing dynamics induced by AS-path prepending, a common method for controlling the inbound traffic of a multi-homed ISP. Unlike other inter-domain inbound traffic engineering methods, AS-path prepending not only provides network resilience but does not increase routing table size. Unfortunately, ISPs often perform prepending on a trail-and-error basis, which can lead to suboptimal results and to a large amount of network churn. We study these effects by actively injecting prepended routes into the Internet routing system using the RIPE NCC RIS route collectors and observing the resulting changes from almost 200 publicly-accessible sources of BGP information. Our results show that our prepending methods are simple and effective and that a small number of ASes is often responsible for large amounts of the route changes caused by prepending. Furthermore, we show that our methods are able to reveal hidden prepending policies to prepending and tie-breaking decisions made by ASes; this is useful for further predicting the behavior of prepending.1},
booktitle = {Proceedings of the 2007 Workshop on Experimental Computer Science},
pages = {20–es},
keywords = {network measurement, inter-domain traffic engineering, border gateway protocol, beacon prefix, as path prepending},
location = {San Diego, California},
series = {ExpCS '07}
}

@article{10.1145/3510415,
author = {Zhong, Zhiheng and Xu, Minxian and Rodriguez, Maria Alejandra and Xu, Chengzhong and Buyya, Rajkumar},
title = {Machine Learning-based Orchestration of Containers: A Taxonomy and Future Directions},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3510415},
doi = {10.1145/3510415},
abstract = {Containerization is a lightweight application virtualization technology, providing high environmental consistency, operating system distribution portability, and resource isolation. Existing mainstream cloud service providers have prevalently adopted container technologies in their distributed system infrastructures for automated application management. To handle the automation of deployment, maintenance, autoscaling, and networking of containerized applications, container orchestration is proposed as an essential research problem. However, the highly dynamic and diverse feature of cloud workloads and environments considerably raises the complexity of orchestration mechanisms. Machine learning algorithms are accordingly employed by container orchestration systems for behavior modeling and prediction of multi-dimensional performance metrics. Such insights could further improve the quality of resource provisioning decisions in response to the changing workloads under complex environments. In this article, we present a comprehensive literature review of existing machine learning-based container orchestration approaches. Detailed taxonomies are proposed to classify the current researches by their common features. Moreover, the evolution of machine learning-based container orchestration technologies from the year 2016 to 2021 has been designed based on objectives and metrics. A comparative analysis of the reviewed techniques is conducted according to the proposed taxonomies, with emphasis on their key characteristics. Finally, various open research challenges and potential future directions are highlighted.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {217},
numpages = {35},
keywords = {systematic review, resource provisioning, cloud computing, machine learning, Container orchestration}
}

@article{10.1109/TNET.2020.2986252,
author = {Sarker, Ankur and Qiu, Chenxi and Shen, Haiying},
title = {Connectivity Maintenance for Next-Generation Decentralized Vehicle Platoon Networks},
year = {2020},
issue_date = {Aug. 2020},
publisher = {IEEE Press},
volume = {28},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2986252},
doi = {10.1109/TNET.2020.2986252},
abstract = {Always keeping a certain distance between vehicles in a platoon system is important for collision avoidance. Centralized platoon systems let the leader vehicle determine and notify the velocities of all the vehicles in the platoon. Unfortunately, such a centralized method generates high packet drop rate and communication delay due to the leader vehicle's limited communication capability. Therefore, we propose a decentralized platoon network, in which each vehicle determines its velocity by only communicating with the vehicles in a short range. However, the multiple simultaneous transmissions between different pairs of vehicles may interfere with each other. By leveraging a typical feature of a platoon, we devise a channel allocation algorithm, called the Fast and Lightweight Autonomous channel selection algorithm (FLA), in which each vehicle determines its channel simply based on its distance to the leader vehicle. We also devise a strategy, in which a succeeding vehicle uses its stored common velocity profile when it is disconnected from its preceding vehicle and then adjusts its velocity once the connection is built. We conduct experiments on NS-3 and Matlab to evaluate the performance of our proposed methods and implement a real-world prototype by equipping vehicles with Android mobile devices. The experimental results demonstrate the superior performance of our decentralized platoon network over the previous centralized platoon networks.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {1449–1462},
numpages = {14}
}

@inproceedings{10.1145/1314418.1314431,
author = {Lee, Hannah K. and Luedemann, Heiko},
title = {lightweight decentralized authorization model for inter-domain collaborations},
year = {2007},
isbn = {9781595938923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1314418.1314431},
doi = {10.1145/1314418.1314431},
abstract = {Inter-domain collaborations comprise of a series of tasks, whose run-time environment stretches over heterogeneous systems governed by different set of policies and where participating organizations desire to preserve control over their resources. One of the major security challenges in modeling those inter-domain collaborations is providing a decentralized authorization solution. At the core of this challenge lie two problems: 1) an authorization decision maker does not know who a principal is and 2) which set of privileges this principal owns if the principal is originated from outside of the decision maker's domain. Currently, a number of different approaches tackle this problem and claim to provide a full-fledged solution. These approaches, however, often require particular use of infrastructures and their own policy languages. In this paper, we propose a lightweight model using the concept of distributed roles from the dRBAC model to bridge different domain boundaries. Based on e-Government collaboration scenarios, we identify a set of requirements of decentralized authorization and propose an extension to the current XACML specification as a realization of our model.},
booktitle = {Proceedings of the 2007 ACM Workshop on Secure Web Services},
pages = {83–89},
numpages = {7},
keywords = {inter-domain collaborations, e-government, dRBAC, authorization policy, XACML},
location = {Fairfax, Virginia, USA},
series = {SWS '07}
}

@inproceedings{10.1145/3457337.3457842,
author = {Morsbach, Felix and Toor, Salman},
title = {DecFL: An Ubiquitous Decentralized Model Training Protocol and Framework Empowered by Blockchain},
year = {2021},
isbn = {9781450384001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457337.3457842},
doi = {10.1145/3457337.3457842},
abstract = {Machine learning has become ubiquitous across many fields in the last decade and modern real world applications often require a decentralized solution for training such models. This demand sprouted the research in federated learning, which solves some of the challenges with centralized machine learning, but at the same times raises further questions in regard to security, privacy and scalability. We have designed and implemented DecFL, an ubiquitous protocol for decentralized model training. The protocol is machine-learning-model-, vendor-, and technology-agnostic and provides a basis for practitioner's own implementations. The implemented DecFL framework presented in this article is an exemplary realization of the carefully designed protocol stack based on Ethereum and IPFS and offers a scalable baseline solution for decentralized machine learning. In this article, we present a study based on the proposed protocol, its theoretical bounds and experiments based on the implemented framework. Using open-source datasets (MNIST and CIFAR10), we demonstrate key features, the actual cost of training a model (in euro) and the communication overhead. We further show that through a proper choice of technologies DecFL achieves a linear scaling, which is a non-trivial task in a decentralized setting. Along with discussing some of the security challenges in the field, we highlight aggregation poisoning as a relevant attack vector, its associated risks and a possible prevention strategy for decentralized model training through DecFL.},
booktitle = {Proceedings of the 3rd ACM International Symposium on Blockchain and Secure Critical Infrastructure},
pages = {61–70},
numpages = {10},
keywords = {security, privacy, federated machine learning, decentralized model training, communication protocol, blockchain},
location = {Virtual Event, Hong Kong},
series = {BSCI '21}
}

@inproceedings{10.1145/3545948.3545977,
author = {Shi, Yang and Liang, Junqing and Li, Mianhong and Ma, Tianchen and Ye, Guodong and Li, Jiangfeng and Zhao, Qinpei},
title = {Threshold EdDSA Signature for Blockchain-based Decentralized Finance Applications},
year = {2022},
isbn = {9781450397049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545948.3545977},
doi = {10.1145/3545948.3545977},
abstract = {The threshold digital signature technique is important for decentralized finance (DeFi) applications such as asset custody and cross-chain interoperations. The Edwards-curve digital signature algorithm (EdDSA) is widely used in blockchains, e.g., Libra/Diem; however, no suitable threshold solution exists. Therefore, to bridge this gap, we propose a threshold EdDSA that allows n parties to generate keys in a decentralized and distributed manner. Any t + 1-of-n parties can generate standard EdDSA signatures. This scheme supports an arbitrary threshold (t, n) and has been proven to be secure against at most t malicious adversaries. The theoretical analysis (computation complexity and communication footprints) and experimental results demonstrate that the proposed scheme performs efficiently on cloud servers and embedded devices. Furthermore, the proposed scheme is integrated with Tendermint, a blockchain framework that uses EdDSA, to generate keys and sign transactions in a decentralized manner, which indicates that this scheme is compatible with blockchains for supporting DeFi applications.},
booktitle = {Proceedings of the 25th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {129–142},
numpages = {14},
keywords = {threshold signature, decentralized finance, cross-chain, blockchain, asset custody, EdDSA},
location = {Limassol, Cyprus},
series = {RAID '22}
}

@inproceedings{10.1145/3297662.3365815,
author = {Sangar, Disha and Haugerud, H\r{a}rek and Yazidi, Anis and Begnum, Kyrre},
title = {A Decentralized Approach for Homogenizing Load Distribution: In Cloud Data Center Based on Stable Marriage Matching},
year = {2020},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365815},
doi = {10.1145/3297662.3365815},
abstract = {Running a sheer virtualized data center with the help of Virtual Machines (VM) is the de facto-standard in modern data centers. Live migration offers immense flexibility opportunities as it endows the system administrators with tools to seamlessly move VMs across physical machines. Several studies have shown that the resource utilization within a data center is not homogeneous across the physical servers. Load imbalance situations are observed where a significant portion of servers are either in overloaded or underloaded states. Apart from leading to inefficient usage of energy by underloaded servers, this might lead to serious QoS degradation issues in the overloaded servers.In this paper, we propose a lightweight decentralized solution for homogenizing the load across different machines in a data center. In search of better solutions, we have looked outside the field of computer science for inspiration. Inspired by Nobel Peace Prize winners Alvin Roth and Lloyd Shapley's work on Stable Matching [4], we borrow the concept of stable marriage matching problems where we pair pairs of underloaded servers and overloaded servers based on some notion of preferences for the purpose of homogenizing their load through exchange of VMs. Furthermore, our solution is distributed by accommodating this aspect in the original Stable Matching algorithm. We provide some real-life experimental results that demonstrate the efficiency of our approach.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {292–299},
numpages = {8},
keywords = {cloud computing, Stable Marriage, Self-Organization, Distributed Load Balancing},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@inproceedings{10.1109/ASONAM55673.2022.10068697,
author = {Lima, In\^{e}s Rito and Marinho, Claudia and Filipe, Vasco and Ulisses, Alexandre and Saurabh, Nishant and Chakravorty, Antorweep and Zhao, Zhiming and Hristov, Atanas and Prodan, Radu},
title = {MOGPlay: A Decentralized Crowd Journalism Application for Democratic News Production},
year = {2023},
isbn = {9781665456616},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM55673.2022.10068697},
doi = {10.1109/ASONAM55673.2022.10068697},
abstract = {Media production and consumption behaviors are changing in response to new technologies and demands, giving birth to a new generation of social applications. Among them, crowd journalism represents a novel way of constructing democratic and trustworthy news relying on ordinary citizens arriving at breaking news locations and capturing relevant videos using their smartphones. The ARTICONF project [1] proposes a trustworthy, resilient, and globally sustainable toolset for developing decentralized applications (DApps). Leveraging the ARTICONF tools, we introduce a new DApp for crowd journalism called MOGPlay. MOGPlay collects and manages audio-visual content generated by citizens and provides a secure blockchain platform that rewards all stakeholders involved in professional news production. Besides live streaming, MOGPlay offers a marketplace for audio-visual content trading among citizens and free journalists with an internal token ecosystem. We discuss the functionality and implementation of the MOGPlay DApp and illustrate three pilot crowd journalism live scenarios that validate the prototype.},
booktitle = {Proceedings of the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {462–469},
numpages = {8},
keywords = {marketplace, social media, decentralized app, citizen-generated content, crowd journalism},
location = {Istanbul, Turkey},
series = {ASONAM '22}
}

@inproceedings{10.1145/3412841.3441907,
author = {Chiu, Wei-Yang and Meng, Weizhi},
title = {Towards decentralized bicycle insurance system based on blockchain},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441907},
doi = {10.1145/3412841.3441907},
abstract = {Having a transparent and convenient insurance system is desired by users, but the insurance industry currently plays differently. For example, when users ask for a fast and simple self-service, the industry may need a long and cumbersome process, and sometimes users are not sure whether they have a fair deal according to their contract. With the aim of enhancing the current bicycle insurance system in Denmark, we investigate the local market and develop BlockCycle, which is a blockchain-based bicycle insurance system. The basic idea is that insurance companies transform their insurance products into a smart contract, including the criteria of applying the product and the eligibility of compensating into program logic. When a smart contract is deployed, the immutability can ensure the deal integrity and protect both users and insurance companies from the threat of unauthorized manipulation. In addition, blockchain can provide a secure and simple way to decentralize data and services, which enables high availability and reliability. Our experimental results indicate the viability of our system.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {249–256},
numpages = {8},
keywords = {smart contract, decentralized application, data integrity, blockchain, bicycle insurance},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.5555/2499406.2499418,
author = {Nobre, J\'{e}ferson C. and Granville, Lisandro Z. and Clemm, Alexander and Prieto, Alberto Gonzalez},
title = {Decentralized detection of SLA violations using P2P technology},
year = {2012},
isbn = {9781450322102},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {Critical networked services enable significant revenue for network operators and, in turn, are regulated by Service Level Agreements (SLAs). In order to ensure SLAs are being met, service levels need to be monitored. One technique for this involves active measurements, such as IPSLA. However, active measurements are expensive in terms of CPU consumption on network devices. As a result, active measurements usually can cover only a fraction of what could be measured, which can lead to SLA violations being missed. The definition of which subsets of service paths to measure and to configure corresponding measurement probes is a practice that does not scale well and results in fairly static measurement setups that do not adapt well to shifting networking patterns. We propose a solution to increase the detection rate of SLA violations in which devices in a network autonomously and dynamically determine how to place probes in order to detect service level violations. It does not require human intervention, is adaptive to changes in network conditions, resilient to networking faults, and independent of the underlying active measurement technology. Our solution is based on peer-to-peer principles and is characterized by a high degree of decentralized decision making across a network using a self-organizing overlay. In these experiments it is possible to observe that an increase in the information used in probe placement decisions decreases the number of SLA violations missed.},
booktitle = {Proceedings of the 8th International Conference on Network and Service Management},
pages = {100–107},
numpages = {8},
location = {Las Vegas, Nevada},
series = {CNSM '12}
}

@inproceedings{10.5555/2772879.2773357,
author = {Lin, Lanny and Goodrich, Michael A.},
title = {Sliding Autonomy for UAV Path-Planning: Adding New Dimensions to Autonomy Management},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Increased use of autonomy also increases the need for humans to interact with or manage autonomy. We propose a new variation of sliding autonomy useful for planning problems over a spatial region. With this approach, the user can influence the behavior of the autonomous system via spatial constraints and temporal constraints. We present a set of user interface designs to implement sliding autonomy for Unmanned Aerial Vehicle path-planning to support Wilderness Search and Rescue. Interactivities along these new dimensions allow the user to allocate degrees of authority and flexibility to the robot's algorithms. We evaluate the usefulness of the approach against manual and simple pattern path-planning methods with a user study. Results show that the sliding autonomy approach performs significantly better than the other two methods without increasing the users' mental workload, and the performance of the human-autonomy team outperforms either human or autonomy working alone.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1615–1624},
numpages = {10},
keywords = {supervisory control, sliding autonomy, path-planning, human-robot interaction, adjustable autonomy},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1145/3575757.3593644,
author = {Maile, Lisa and Voitlein, Dominik and Grigorjew, Alexej and Hielscher, Kai-Steffen J. and German, Reinhard},
title = {On the Validity of Credit-Based Shaper Delay Guarantees in Decentralized Reservation Protocols},
year = {2023},
isbn = {9781450399838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575757.3593644},
doi = {10.1145/3575757.3593644},
abstract = {Resource reservation is a fundamental mechanism for ensuring quality of service in time-sensitive networks, which can be decentralized by using reservation protocols. In the Ethernet technology Time-Sensitive Networking, this has been proposed in conjunction with the Credit-Based Shaper. For the reservation, the standards assume a maximum worst-case latency bound at each hop. However, we will show through formal analysis and simulation that these worst-case latency bounds are not safe. To face this, we propose an extension to the current standards to allow the reservation of time-sensitive traffic with reliable latency guarantees. The effectiveness of our approach is demonstrated through simulations of both synthetic and industrial networks. Finally, by providing additional information about neighboring devices, we could further increase the maximum reservable traffic by up to 20\% in our test cases.},
booktitle = {Proceedings of the 31st International Conference on Real-Time Networks and Systems},
pages = {108–118},
numpages = {11},
keywords = {Time-Sensitive Networking, Resource Allocation, Reservation Protocol, Network Calculus, Latency, Decentralized Network},
location = {Dortmund, Germany},
series = {RTNS '23}
}

@inproceedings{10.5555/3635637.3663039,
author = {Song, Yan and Jiang, He and Zhang, Haifeng and Tian, Zheng and Zhang, Weinan and Wang, Jun},
title = {Boosting Studies of Multi-Agent Reinforcement Learning on Google Research Football Environment: The Past, Present, and Future},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Even though Google Research Football (GRF) was initially benchmarked and studied as a single-agent environment in its original paper [19], recent years have witnessed an increasing focus on its multi-agent nature by researchers utilizing it as a testbed for Multi-Agent Reinforcement Learning (MARL), especially in the cooperative scenarios. However, the absence of standardized environment settings and unified evaluation metrics for multi-agent scenarios hampers the consistent understanding of various studies. Furthermore, the challenging 5 vs 5 and 11 vs 11 full-game scenarios have received limited thorough examination due to their substantial training complexities. To address these gaps, this paper extends the original environment by not only standardizing the environment settings and benchmarking cooperative learning algorithms across different scenarios, including the most challenging full-game scenarios, but also by discussing approaches to enhance football AI from diverse perspectives and introducing related research tools for learning beyond multi-agent cooperation. Specifically, we provide a distributed and asynchronous population-based self-play framework with diverse pre-trained policies for faster training, two football-specific analytical tools for deeper investigation, and an online leaderboard for broader evaluation. The overall expectation of this work is to advance the study of Multi-Agent Reinforcement Learning both on and with Google Research Football environment, with the ultimate goal of deploying these technologies to real-world applications, such as sports analysis.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {1772–1781},
numpages = {10},
keywords = {agent coordination, multi-agent reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.1145/2830629.2830648,
author = {Tomlinson, Bill and Nardi, Bonnie and Patterson, Donald J. and Raturi, Ankita and Richardson, Debra and Saphores, Jean-Daniel and Stokols, Dan},
title = {Toward Alternative Decentralized Infrastructures},
year = {2015},
isbn = {9781450334907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2830629.2830648},
doi = {10.1145/2830629.2830648},
abstract = {New forms of infrastructure are needed in a world characterized by the burdens of global climate change, a growing population, increasing socio-technical complexity, and natural and human stressors to our human systems. Enabling communities to transition to a more resilient configuration of infrastructures is crucial for establishing a distributed portfolio of processes and systems by which human needs may be met. This paper proposes a potential way to increase infrastructure resilience by supporting the creation of alternative, decentralized infrastructures (ADIs) composed of small-scale, heterogeneous systems and processes. We see two possible roles for these ADIs: first, they could be integrated with existing infrastructures in the industrialized world, thereby providing some redundancy during times of strain on larger centralized systems; and second, they could help developing communities leapfrog centralized and more capital intensive conventional infrastructure. We present a model for how ADI systems may be built, based on principles from software engineering. Finally, we identify some challenges that go beyond technical implementation details in the instantiation of ADIs, and offer some thoughts on how to address them.},
booktitle = {Proceedings of the 2015 Annual Symposium on Computing for Development},
pages = {33–40},
numpages = {8},
keywords = {sustainability, software engineering, infrastructure, ict4d},
location = {London, United Kingdom},
series = {DEV '15}
}

@inproceedings{10.5555/3199700.3199844,
author = {Chhetri, Sujit Rokka and Rashid, Nafiul and Faezi, Sina and Faruque, Mohammad Abdullah Al},
title = {Security trends and advances in manufacturing systems in the era of industry 4.0},
year = {2017},
publisher = {IEEE Press},
abstract = {The next industrial revolution will incorporate various enabling technologies. These technologies will make the product lifecycle of the manufacturing system efficient, decentralized, and well-connected. However, these technologies have various security issues, and when integrated in the product lifecycle of manufacturing systems can pose various challenges for maintaining the security requirements such as confidentiality, integrity, and availability. In this paper, we will present the various trends and advances in the security of the product lifecycle of the manufacturing system while highlighting the roles played by the major enabling components of Industry 4.0.},
booktitle = {Proceedings of the 36th International Conference on Computer-Aided Design},
pages = {1039–1046},
numpages = {8},
location = {Irvine, California},
series = {ICCAD '17}
}

@inproceedings{10.1145/1089761.1089776,
author = {Olariu, Stephan and Eltoweissy, Mohamed and Younis, Mohamed},
title = {ANSWER: autonomous wireless sensor network},
year = {2005},
isbn = {1595932410},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1089761.1089776},
doi = {10.1145/1089761.1089776},
abstract = {The main contribution of this work is to propose a new concept: the AutoNomouS Wireless sEnsor netwoRk (ANSWER) whose mission is to provide in-situ users with secure information that enhances their context awareness. ANSWER finds immediate applications to both overt and covert operations ranging from tactical battlefield surveillance to crisis management and homeland security. ANSWER is capable of performing sophisticated analyses for detecting trends and identifying unexpected, coherent, and emergent behavior.},
booktitle = {Proceedings of the 1st ACM International Workshop on Quality of Service \&amp; Security in Wireless and Mobile Networks},
pages = {88–95},
numpages = {8},
keywords = {wireless security, autonomous sensor networks, anonymity},
location = {Montreal, Quebec, Canada},
series = {Q2SWinet '05}
}

@article{10.1145/3641120,
author = {Gudmundsson, Jens and Hougaard, Jens Leth},
title = {Blockchain-based Decentralized Reward Sharing: The Case of Mining Pools},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {2167-8375},
url = {https://doi.org/10.1145/3641120},
doi = {10.1145/3641120},
abstract = {We introduce a reciprocity protocol, an innovative approach to coordinating and sharing rewards in blockchains. Inherently decentralized and implementable, it puts emphasis on incentives rather than forcing specific sharing rules from the outset. Analyzing the non-cooperative game the protocol induces, we identify a robust, strict, and Pareto-dominant symmetric equilibrium. In it, even self-centered participants show extensive systemic reciprocity. Thus, despite a setting that is generally unfavorable to reciprocal behavior, the protocol enables users to build trust between themselves by taking on a role akin to a social contract.},
journal = {ACM Trans. Econ. Comput.},
month = {mar},
articleno = {4},
numpages = {26},
keywords = {Blockchain, reciprocity, protocol design, Nash equilibrium}
}

@inproceedings{10.1145/3290605.3300448,
author = {Auferbauer, Daniel and Tellio\u{g}lu, Hilda},
title = {Socio-technical Dynamics: Cooperation of Emergent and Established Organisations in Crises and Disasters},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300448},
doi = {10.1145/3290605.3300448},
abstract = {Increasing ubiquitousness of information and communication technology exerts influence on crisis and disaster management. New media enable citizens to rapidly self-organise in emergent groups. Theoretical framing of their interactions with established organisations is lacking. To address this, we conduct a thematic analysis on qualitative data from the European migration crisis of 2015. We draw on context-rich material from both emergent groups and established organisation. To represent our findings, we introduce the notion of socio-technical dynamics. We derive implications for computer supported cooperative work in crises and disasters. These insights contribute to the efficient involvement of emergent groups in established systems.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {volunteers, technology support, qualitative study, crisis and disaster management, community engagement},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3510487.3510499,
author = {Bekemeier, Felix},
title = {Deceptive Assurance? A Conceptual View on Systemic Risk in Decentralized Finance (DeFi)},
year = {2022},
isbn = {9781450387460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510487.3510499},
doi = {10.1145/3510487.3510499},
abstract = {The Decentralized Finance (DeFi) ecosystem has recently been touted as a potential replacement for the existing financial system, with the monetary equivalent in this ecosystem based on various token concepts and infrastructural protocols. However, questions remain regarding the systemic risk of this ecosystem, and closer examination reveals interesting parallels to the concept of systemic risk in established financial systems. There is a need for research to examine important additional dimensions in relation to DeFi. This paper addresses systemic risk in DeFi, presenting the first holistic research framework on the topic, as well as the first empirical indications in order to create foundations for further research.},
booktitle = {Proceedings of the 2021 4th International Conference on Blockchain Technology and Applications},
pages = {76–87},
numpages = {12},
keywords = {Systemic risk, Decentralized Finance, Blockchain technology},
location = {Xi'an, China},
series = {ICBTA '21}
}

@article{10.1145/3441692,
author = {Huang, Huawei and Kong, Wei and Zhou, Sicong and Zheng, Zibin and Guo, Song},
title = {A Survey of State-of-the-Art on Blockchains: Theories, Modelings, and Tools},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3441692},
doi = {10.1145/3441692},
abstract = {To draw a roadmap of current research activities of the blockchain community, we first conduct a brief overview of state-of-the-art blockchain surveys published in the past 5 years. We found that those surveys are basically studying the blockchain-based applications, such as blockchain-assisted Internet of Things (IoT), business applications, security-enabled solutions, and many other applications in diverse fields. However, we think that a comprehensive survey toward the essentials of blockchains by exploiting the state-of-the-art theoretical modelings, analytic models, and useful experiment tools is still missing. To fill this gap, we perform a thorough survey by identifying and classifying the most recent high-quality research outputs that are closely related to the theoretical findings and essential mechanisms of blockchain systems and networks. Several promising open issues are also summarized for future research directions. We hope this survey can serve as a useful guideline for researchers, engineers, and educators about the cutting-edge development of blockchains in the perspectives of theories, modelings, and tools.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {44},
numpages = {42},
keywords = {theoretical modelings, experiment tools, analytic models, Blockchain}
}

@article{10.1145/1964897.1964919,
author = {Becchetti, Luca and Bordino, Ilaria and Leonardi, Stefano and Rosen, Adi},
title = {Fully decentralized computation of aggregates over data streams},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/1964897.1964919},
doi = {10.1145/1964897.1964919},
abstract = {In several emerging applications, data is collected in massive streams at several distributed points of observation. A basic and challenging task is to allow every node to monitor a neighbourhood of interest by issuing continuous aggregate queries on the streams observed in its vicinity. This class of algorithms is fully decentralized and diffusive in nature: collecting all data at few central nodes of the network is unfeasible in networks of low capability devices or in the presence of massive data sets.The main difficulty in designing diffusive algorithms is to cope with duplicate detections. These arise both from the observation of the same event at several nodes of the network and/or receipt of the same aggregated information along multiple paths of diffusion. In this paper, we consider fully decentralized algorithms that answer locally continuous aggregate queries on the number of distinct events, total number of events and the second frequency moment in the scenario outlined above. The proposed algorithms use in the worst case or on realistic distributions sublinear space at every node. We also propose strategies that minimize the communication needed to update the aggregates when new events are observed. We experimentally evaluate for the efficiency and accuracy of our algorithms on realistic simulated scenarios.},
journal = {SIGKDD Explor. Newsl.},
month = {mar},
pages = {83–91},
numpages = {9}
}

@inproceedings{10.1145/3319535.3354253,
author = {Sun, Haipei and Xiao, Xiaokui and Khalil, Issa and Yang, Yin and Qin, Zhan and Wang, Hui (Wendy) and Yu, Ting},
title = {Analyzing Subgraph Statistics from Extended Local Views with Decentralized Differential Privacy},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3354253},
doi = {10.1145/3319535.3354253},
abstract = {Many real-world social networks are decentralized in nature, and the only way to analyze such a network is to collect local views of the social graph from individual participants. Since local views may contain sensitive information, it is often desirable to apply differential privacy in the data collection process, which provides strong and rigorous privacy guarantees. In many practical situations, the local view of a participant contains not only her own connections, but also those of her neighbors, which are private and sensitive for the neighbors, but not directly so for the participant herself. We call such information beyond direct connections an extended local view (ELV)&lt;/&gt;, and study two fundamental problems related to ELVs: first, how do we correctly enforce differential privacy for all participants in the presence of ELVs? Second, how can the data collector utilize ELVs to obtain accurate estimates of global graph properties?This paper points out that when collecting ELVs, it is insufficient to apply a straightforward adaptation of local differential privacy (LDP)&lt;/&gt;, a commonly used scheme in practice, to protect the privacy of all network participants. The main problem is that an adversarial data collector can accumulate private information on a specific victim from multiple neighbors of the victim; even though the data collected from each neighbor is perturbed under LDP, their aggregate can still violate the victim's privacy. To prevent this attack, we formulate a novel decentralized differential privacy (DDP)&lt;/&gt; scheme, which requires that each participant consider not only her own privacy, but also that of her neighbors involved in her ELV.The stringent privacy requirement of DDP, however, makes it challenging to design an effective mechanism for data collection. Towards this goal, we design a novel multi-phase framework under DDP&lt;/&gt; that enables an analyst to accurately estimate subgraph counts, an important property of social graphs. The main idea is that instead of collecting subgraph counts directly, which would require excessively noise, the analyst first asks individuals about their respective minimum noise scale, which is private information since it depends on the local graph structure, and, thus, must be performed under DDP. For some types of subgraphs, this process is applied recursively&lt;/&gt;, i.e., the analyst asks about the necessary noise to be injected into the private information on the minimum local noise scale required to protect subgraph counts under DDP. As case studies, we instantiate the proposed framework for three common subgraph patterns: triangles, three-hop paths, and k&lt;/&gt;-cliques. Extensive experiments using real data demonstrate that the proposed scheme leads to accurate estimates of global subgraph counts, whereas baseline solutions fail to obtain meaningful result utility.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {703–717},
numpages = {15},
keywords = {subgraph statistics, social networks, decentralized differential privacy},
location = {London, United Kingdom},
series = {CCS '19}
}

@article{10.1145/3127499,
author = {Ferroni, Matteo and Corna, Andrea and Damiani, Andrea and Brondolin, Rolando and Kubiatowicz, John D. and Sciuto, Donatella and Santambrogio, Marco D.},
title = {MARC: A Resource Consumption Modeling Service for Self-Aware Autonomous Agents},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3127499},
doi = {10.1145/3127499},
abstract = {Autonomicity is a golden feature when dealing with a high level of complexity. This complexity can be tackled partitioning huge systems in small autonomous modules, i.e., agents. Each agent then needs to be capable of extracting knowledge from its environment and to learn from it, in order to fulfill its goals: this could not be achieved without proper modeling techniques that allow each agent to gaze beyond its sensors. Unfortunately, the simplicity of agents and the complexity of modeling do not fit together, thus demanding for a third party to bridge the gap.Given the opportunities in the field, the main contributions of this work are twofold: (1) we propose a general methodology to model resource consumption trends and (2) we implemented it into MARC, a Cloud-service platform that produces Models-as-a-Service, thus relieving self-aware agents from the burden of building their custom modeling framework. In order to validate the proposed methodology, we set up a custom simulator to generate a wide spectrum of controlled traces: this allowed us to verify the correctness of our framework from a general and comprehensive point of view.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {nov},
articleno = {21},
numpages = {29},
keywords = {resource consumption, discrete Markov models, autoregressive with exogenous variable models, Model-as-a-service}
}

@article{10.1145/2894750,
author = {To, Quoc-Cuong and Nguyen, Benjamin and Pucheral, Philippe},
title = {Private and Scalable Execution of SQL Aggregates on a Secure Decentralized Architecture},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/2894750},
doi = {10.1145/2894750},
abstract = {Current applications, from complex sensor systems (e.g., quantified self) to online e-markets, acquire vast quantities of personal information that usually end up on central servers where they are exposed to prying eyes. Conversely, decentralized architectures that help individuals keep full control of their data complexify global treatments and queries, impeding the development of innovative services. This article aims precisely at reconciling individual's privacy on one side and global benefits for the community and business perspectives on the other. It promotes the idea of pushing the security to secure hardware devices controlling the data at the place of their acquisition. Thanks to these tangible physical elements of trust, secure distributed querying protocols can reestablish the capacity to perform global computations, such as Structured Query Language (SQL) aggregates, without revealing any sensitive information to central servers. This article studies how to secure the execution of such queries in the presence of honest-but-curious and malicious attackers. It also discusses how the resulting querying protocols can be integrated in a concrete decentralized architecture. Cost models and experiments on SQL/Asymmetric Architecture (AA), our distributed prototype running on real tamper-resistant hardware, demonstrate that this approach can scale to nationwide applications.},
journal = {ACM Trans. Database Syst.},
month = {aug},
articleno = {16},
numpages = {43},
keywords = {parallel computing, decentralized architecture, Trusted hardware}
}

@inproceedings{10.1145/3643915.3644081,
author = {Vilchez, Enrique and Troya, Javier and Camara, Javier},
title = {Towards Proactive Decentralized Adaptation of Unmanned Aerial Vehicles for Wildfire Tracking},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644081},
doi = {10.1145/3643915.3644081},
abstract = {Smart Cyber-Physical Systems (sCPS) operate in dynamic and uncertain environments, where anticipation to adverse situations is crucial and decentralization is often necessary due to e.g., scalability issues. Addressing the limitations related to the lack of foresight of (decentralized) reactive self-adaptation (e.g., slower response, sub-optimal resource usage), this paper introduces a novel method that employs Predictive Coordinate Descent (PCD) to enable decentralized proactive self-adaptation in sCPS. Our study compares PCD with a reactive Deep Q-Network (DQN) strategy on Unmanned Aerial Vehicles (UAV) in wildfire tracking adaptation scenarios. Results show how PCD outperforms DQN when furnished with high-quality predictions of the environment, but progressively degrades in effectiveness with predictions of decreasing quality.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {56–62},
numpages = {7},
keywords = {UAV, proactive adaptation, predictive coordinate descent},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.5555/800240.807177,
author = {Redwood, P. H.S. and Schengili, J. J.},
title = {Simulation in a decentralized planning environment},
year = {1970},
publisher = {Winter Simulation Conference},
abstract = {The problem of integrating and controlling the planning process in a decentralized corporation—without stifling the individuality of its component units—is attacked through the use of simulation. A system of models, each representing a distinct organizational entity, permits simulation in detail to take place concurrently at several levels, using guidelines and criteria set by the central authority. The provision of a common planning data base, to which each of the models is linked, facilitates the monitoring of plans and permits the allocation of resources which best meets the objectives of the corporation as a whole.},
booktitle = {Proceedings of the Fourth Annual Conference on Applications of Simulation},
pages = {8–11},
numpages = {4},
location = {New York, New York, USA}
}

@article{10.1145/358690.358720,
author = {Olson, Margrethe H. and Lucas, Henry C.},
title = {The impact of office automation on the organization: some implications for research and practice},
year = {1982},
issue_date = {Nov 1982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/358690.358720},
doi = {10.1145/358690.358720},
abstract = {Computer technology has recently been applied to the automation of office tasks and procedures. Much of the technology is aimed not at improving the efficiency of current office procedures, but at altering the nature of office work altogether. The development of automated office systems raises a number of issues for the organization. How will this technology be received by organization members? How will it affect the definition of traditional office work? What will be its impact on individuals, work groups, and the structure of the organization? This paper presents a descriptive model and propositions concerning the potential impacts of office automation on the organization and it stresses the need, when implementing automated office systems, to take a broad perspective of their potential positive and negative effects on the organization. The need for further research examining the potential effects of office automation is emphasized.},
journal = {Commun. ACM},
month = {nov},
pages = {838–847},
numpages = {10},
keywords = {office automation, impact on organizations, electronic mail, automated office systems}
}

@inproceedings{10.1145/3544548.3581173,
author = {Fan, Sizheng and Min, Tian and Wu, Xiao and Cai, Wei},
title = {Altruistic and Profit-oriented: Making Sense of Roles in Web3 Community from Airdrop Perspective},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581173},
doi = {10.1145/3544548.3581173},
abstract = {Regardless of which community, incentivizing users is a necessity for well-sustainable operations. In the blockchain-backed Web3 communities, known for their transparency and security, airdrop serves as a widespread incentive mechanism for allocating capital and power. However, it remains a controversy on how to justify airdrop to incentive and empower the decentralized governance. In this paper, we use ParaSwap as an example to propose a role taxonomy methodology through a data-driven study to understand the characteristic of community members and the effectiveness of airdrop. We find that users receive more rewards tend to take positive actions towards the community. We summarize several arbitrage patterns and confirm the current detection is not sufficient in screening out airdrop hunters. In conjunction with the results, we discuss from the aspects of interaction, financialization, and system design to conclude the challenges and possible research directions for decentralized communities.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {551},
numpages = {16},
keywords = {Airdrop, Decentralized community, Network analysis, Unsupervised learning},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3627106.3627110,
author = {Wang, Ke and Gao, Jianbo and Wang, Qiao and Zhang, Jiashuo and Li, Yue and Guan, Zhi and Chen, Zhong},
title = {Hades: Practical Decentralized Identity with Full Accountability and Fine-grained Sybil-resistance},
year = {2023},
isbn = {9798400708862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627106.3627110},
doi = {10.1145/3627106.3627110},
abstract = {Decentralized identity (DID), the idea of giving users complete control over their identity-related data, is being used to solve the privacy tension in the identity management of decentralized applications (Dapps). While existing approaches do an excellent job of solving the privacy tension, they have not adequately addressed the accountability and Sybil-resistance issues. Moreover, these approaches have a considerable gas overhead, making them impractical for Dapps. We presented Hades, a novel practical DID system supporting full accountability and fine-grained Sybil-resistance while providing strong privacy properties. Hades supports three aspects of accountability, i.e., auditability, traceability, and revocation. Hades is the first DID system that supports accountability in all these three aspects. Hades is also the first DID system that supports fine-grained Sybil-resistance, enabling Dapps to customize personalized Sybil resistance strategies based on users’ identity attributes. Hades can run efficiently on the Ethereum Virtual Machine (EVM). We implemented and evaluated Hades. The benchmarks showed that Hades has the lowest gas cost incurred on EVM as far as we know. Also, we presented a case study on attribute-associated fair NFT distribution (“airdrops”) where all previous works failed, whereas we gave a solution leveraging Hades.},
booktitle = {Proceedings of the 39th Annual Computer Security Applications Conference},
pages = {216–228},
numpages = {13},
keywords = {Accountability, Dapp, Decentralized identity, Privacy, Sybil-resistance},
location = {Austin, TX, USA},
series = {ACSAC '23}
}

@inproceedings{10.1145/3589334.3645658,
author = {Zheng, Shuhao and Li, Zonglun and Luo, Junliang and Xin, Ziyue and Liu, Xue},
title = {IDEA-DAC: Integrity-Driven Editing for Accountable Decentralized Anonymous Credentials via ZK-JSON},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645658},
doi = {10.1145/3589334.3645658},
abstract = {Decentralized Anonymous Credential (DAC) systems are increasingly relevant, especially when enhancing revocation mechanisms in the face of complex traceability challenges. This paper introduces IDEA-DAC a paradigm shift from the conventional revoke-and-reissue methods, promoting direct and Integrity-Driven Editing (IDE) for Accountable DACs, which results in better integrity accountability, traceability, and system simplicity. We further incorporate an Edit-bound Conformity Check that ensures tailored integrity standards during credential amendments using R1CS-based ZK-SNARKs. Delving deeper, we propose ZK-JSON, a unique R1CS circuit design tailored for IDE over generic JSON documents. This design imposes strictly O(N) rank-1 constraints for variable-length JSON documents of up to N bytes in length, encompassing serialization, encryption, and edit-bound conformity checks. Additionally, our circuits only necessitate a one-time compilation, setup, and smart contract deployment for homogeneous JSON documents up to a specified size. While preserving core DAC features such as selective disclosure, anonymity, and predicate provability, IDEA-DAC achieves precise data modification checks without revealing private content, ensuring only authorized edits are permitted. In summary, IDEA-DAC offers an enhanced methodology for large-scale JSON-formatted credential systems, setting a new standard in decentralized identity management efficiency and precision.},
booktitle = {Proceedings of the ACM on Web Conference 2024},
pages = {1868–1879},
numpages = {12},
keywords = {decentralized anonymous credential, edit-bound conformity check, integrity-driven editing, zk-json},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3501247.3531597,
author = {Ba, Cheick Tidiane and Michienzi, Andrea and Guidi, Barbara and Zignani, Matteo and Ricci, Laura and Gaito, Sabrina},
title = {Fork-based user migration in Blockchain Online Social Media},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3531597},
doi = {10.1145/3501247.3531597},
abstract = {Nowadays, Online Social Media (OSM) are among the most popular web services. Traditional OSM are known to be affected by serious issues including misinformation, fake news, censorship, and privacy violations, to the point that a pressing demand for new paradigms is raised by users all over the world. Among such paradigms, the concepts around the Web 3.0 are fueling a new revolution of online sociality, pushing towards the adoption of innovative and groundbreaking technologies. In particular, the decentralization of social services through the blockchain technology is representing the most valid alternative to current OSM, enabling the development of rewarding strategies for value redistribution, and fake news detection. However, the so-called Blockchain Online Social Media (BOSMs) are far from being mature, with different platforms that continually try to redefine their services in order to attract larger audiences, thus causing blockchain forks and massive user migrations, with the latter dominating the dynamics of the current OSM landscape, too. In this paper, we deal with the evolution of BOSMs from the perspective of user migration across platforms as a consequence of a fork event. We propose a general user migration model applicable to BOSMs to represent the evolution patterns of fork-based migrations, the multi-interaction structural complexity of BOSMs, and their growth characteristics. Within this framework, we also cope with the task of predicting how users will behave in the case of a fork, i.e. they will remain on the original blockchain or they will migrate to the new one. We apply our framework to the case study of the Steem-Hive fork event, and show the importance of considering both social and economic information, regardless of the learning algorithm considered. To the best of our knowledge, this is the first study on blockchain fork and its related user migration.},
booktitle = {Proceedings of the 14th ACM Web Science Conference 2022},
pages = {174–184},
numpages = {11},
keywords = {Blockchain Online Social Media, Temporal Networks, User Migration},
location = {Barcelona, Spain},
series = {WebSci '22}
}

@article{10.1145/3418685,
author = {Rawat, Abhimanyu and Khodari, Mohammad and Asplund, Mikael and Gurtov, Andrei},
title = {Decentralized Firmware Attestation for In-Vehicle Networks},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3418685},
doi = {10.1145/3418685},
abstract = {Today’s vehicles are examples of Cyber-Physical Systems (CPS) controlled by a large number of electronic control units (ECUs), which manage everything from heating to steering and braking. Due to the increasing complexity and inter-dependency of these units, it has become essential for an ECU to be able to ensure the integrity of the firmware running on other ECU’s to guarantee its own correct operation. Existing solutions for firmware attestation use a centralized approach, which means a single point of failure. In this article, we propose and investigate a decentralized firmware attestation scheme for the automotive domain. The basic idea of this scheme is that each ECU can attest to the state of those ECU’s on which it depends. Two flavors of ECU attestation, i.e., parallel and serial solution, were designed, implemented, and evaluated. The two variants were compared in terms of both detection performance (i.e., the ability to identify unauthorized firmware modifications) and timing performance. Our results show that the proposed scheme is feasible to implement and that the parallel solution showed a significant improvement in timing performance over the serial solution.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {dec},
articleno = {7},
numpages = {23},
keywords = {integrity, firmware, communication system security, attestation, ECU}
}

@article{10.1109/TNET.2009.2020162,
author = {Sharma, Shrutivandana and Teneketzis, Demosthenis},
title = {An externalities-based decentralized optimal power allocation algorithm for wireless networks},
year = {2009},
issue_date = {December 2009},
publisher = {IEEE Press},
volume = {17},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2009.2020162},
doi = {10.1109/TNET.2009.2020162},
abstract = {The rapidly growing demand for wireless communication makes efficient power allocation a critical factor in the network's efficient operation. Power allocation in decentralized wireless systems, where the transmission of a user creates interference to other users and directly affects their utilities, has been recently studied by pricing methods. However, pricing methods do not result in efficient/optimal power allocations for such systems for the following reason. Systems where a user's actions directly affect the utilities of other users are known to have externalities. It is well known from Mas-Colell et al. that in systems with externalities, standard efficiency theorems on market equilibrium do not apply and pricing methods do not result in Pareto optimal outcomes. In this paper, we formulate the power allocation problem for a wireless network as an allocation problem with "externalities." We consider a system where each user knows only its own utility and the channel gains from the transmitters of other users to its own receiver. The system has multiple interference temperature constraints to control interference. We present a decentralized algorithm to allocate transmission powers to the users. The algorithm takes into account the externality generated to the other users by the transmission of each user, satisfies the informational constraints of the system, overcomes the inefficiency of pricing mechanisms and guarantees convergence to globally optimal power allocations.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {1819–1831},
numpages = {13},
keywords = {wireless network, power allocation, microeconomics, mechanism design, interference temperature constraint, interference, externalities, decentralized algorithm}
}

@article{10.1145/3241737,
author = {Buyya, Rajkumar and Srirama, Satish Narayana and Casale, Giuliano and Calheiros, Rodrigo and Simmhan, Yogesh and Varghese, Blesson and Gelenbe, Erol and Javadi, Bahman and Vaquero, Luis Miguel and Netto, Marco A. S. and Toosi, Adel Nadjaran and Rodriguez, Maria Alejandra and Llorente, Ignacio M. and Vimercati, Sabrina De Capitani Di and Samarati, Pierangela and Milojicic, Dejan and Varela, Carlos and Bahsoon, Rami and Assuncao, Marcos Dias De and Rana, Omer and Zhou, Wanlei and Jin, Hai and Gentzsch, Wolfgang and Zomaya, Albert Y. and Shen, Haiying},
title = {A Manifesto for Future Generation Cloud Computing: Research Directions for the Next Decade},
year = {2018},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3241737},
doi = {10.1145/3241737},
abstract = {The Cloud computing paradigm has revolutionised the computer science horizon during the past decade and has enabled the emergence of computing as the fifth utility. It has captured significant attention of academia, industries, and government bodies. Now, it has emerged as the backbone of modern economy by offering subscription-based services anytime, anywhere following a pay-as-you-go model. This has instigated (1) shorter establishment times for start-ups, (2) creation of scalable global enterprise applications, (3) better cost-to-value associativity for scientific and high-performance computing applications, and (4) different invocation/execution models for pervasive and ubiquitous applications. The recent technological developments and paradigms such as serverless computing, software-defined networking, Internet of Things, and processing at network edge are creating new opportunities for Cloud computing. However, they are also posing several new challenges and creating the need for new approaches and research strategies, as well as the re-evaluation of the models that were developed to address issues such as scalability, elasticity, reliability, security, sustainability, and application models. The proposed manifesto addresses them by identifying the major open challenges in Cloud computing, emerging trends, and impact areas. It then offers research directions for the next decade, thus helping in the realisation of Future Generation Cloud Computing.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {105},
numpages = {38},
keywords = {Cloud computing, Cloud economics, Fog computing, InterCloud, application development, data management, scalability, serverless computing, sustainability}
}

@inproceedings{10.1145/1499586.1499755,
author = {Turoff, Murray},
title = {Session on views of the future: chairman's introduction---opposing views},
year = {1973},
isbn = {9781450379168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1499586.1499755},
doi = {10.1145/1499586.1499755},
abstract = {This session represents a "first of a kind" for a major computer conference. The session is devoted entirely to formal technological forecasting and assessment efforts dealing with the computer industry. Technological forecasting as an autonomous discipline, with its own set of methodologies and techniques, is only about five years old. Of course, similar efforts have taken place over the years within the long range planning staffs of most technology-oriented companies and organizations. Furthermore, the intuitive judgment of recognized experts is a technological forecasting technique that has always been with us and has been well represented at these meetings by various panel presentations. What appears to be really new is a growing recognition of the need to examine potential futures systematically in order to assess a wide variety of concerns and potential consequences of technological development. The days of looking only for profit related effects seem to be passing into history. Because the scope of concern has significantly widened, with an accompanying increase in the complexity of the required analyses, new approaches to forecasting have been sought.},
booktitle = {Proceedings of the June 4-8, 1973, National Computer Conference and Exposition},
pages = {717–722},
numpages = {6},
location = {New York, New York},
series = {AFIPS '73}
}

@inproceedings{10.1145/3384943.3409417,
author = {Mir, Omid and Roland, Michael and Mayrhofer, Ren\'{e}},
title = {DAMFA: Decentralized Anonymous Multi-Factor Authentication},
year = {2020},
isbn = {9781450376105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384943.3409417},
doi = {10.1145/3384943.3409417},
abstract = {Token-based authentication is usually applied to enable single-sign-on on the web. In current authentication schemes, users are required to interact with identity providers securely to set up authentication data during a registration phase and receive a token (credential) for future accesses to various services and applications. This type of interaction can make authentication schemes challenging in terms of security and usability. From a security point of view, one of the main threats is the compromisation of identity providers. An adversary who compromises the authentication data (password or biometric) stored with the identity provider can mount an offline dictionary attack. Furthermore, the identity provider might be able to track user activity and control sensitive user data. In terms of usability, users always need a trusted server to be online and available while authenticating to a service provider.In this paper, we propose a new Decentralized Anonymous Multi-Factor Authentication (DAMFA) scheme where the process of user authentication no longer depends on a trusted third party (the identity provider). Also, service and identity providers do not gain access to sensitive user data and cannot track individual user activity. Our protocol allows service providers to authenticate users at any time without interacting with the identity provider.Our approach builds on a Threshold Oblivious Pseudorandom Function (TOPRF) to improve resistance to offline attacks and uses a distributed transaction ledger to improve usability. We demonstrate practicability of our proposed scheme through a prototype.},
booktitle = {Proceedings of the 2nd ACM International Symposium on Blockchain and Secure Critical Infrastructure},
pages = {10–19},
numpages = {10},
keywords = {user anonymity, usability, oblivious pseudorandom function, multi-factor authentication, blockchain},
location = {Taipei, Taiwan},
series = {BSCI '20}
}

@article{10.1145/252829.252833,
author = {Brown, Carol V. and Renwick, Janet S.},
title = {Alignment of the IS organization: the special case of corporate acquisitions},
year = {1996},
issue_date = {Fall 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {0095-0033},
url = {https://doi.org/10.1145/252829.252833},
doi = {10.1145/252829.252833},
journal = {SIGMIS Database},
month = {sep},
pages = {25–33},
numpages = {9},
keywords = {organization theory, mergers and acquisitions, IS management, IS centralization/decentralization}
}

